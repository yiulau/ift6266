{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano\n",
    "theano.sandbox.cuda.dnn.dnn_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks import serialization\n",
    "with open('7_dogvcats.pkl','rb') as f:\n",
    "    o = serialization.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tarball = tarfile.open('7_dogvcats.pkl','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log = tarball.getmember('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tarfile.ExFileObject at 0x7f66000e2150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarball.extractfile(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_log = numpy.load(tarball.extractfile(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': {'_epoch_ends': [625,\n",
       "   1250,\n",
       "   1875,\n",
       "   2500,\n",
       "   3125,\n",
       "   3750,\n",
       "   4375,\n",
       "   5000,\n",
       "   5625,\n",
       "   6250,\n",
       "   6875,\n",
       "   7500,\n",
       "   8125,\n",
       "   8750,\n",
       "   9375,\n",
       "   10000,\n",
       "   10625,\n",
       "   11250,\n",
       "   11875,\n",
       "   12500,\n",
       "   13125,\n",
       "   13750,\n",
       "   14375,\n",
       "   15000,\n",
       "   15625,\n",
       "   16250,\n",
       "   16875,\n",
       "   17500,\n",
       "   18125,\n",
       "   18750,\n",
       "   19375,\n",
       "   20000,\n",
       "   20625,\n",
       "   21250,\n",
       "   21875,\n",
       "   22500,\n",
       "   23125,\n",
       "   23750,\n",
       "   24375,\n",
       "   25000,\n",
       "   25625,\n",
       "   26250,\n",
       "   26875,\n",
       "   27500,\n",
       "   28125,\n",
       "   28750,\n",
       "   29375,\n",
       "   30000,\n",
       "   30625,\n",
       "   31250],\n",
       "  'batch_interrupt_received': False,\n",
       "  'epoch_interrupt_received': False,\n",
       "  'epoch_started': False,\n",
       "  'epochs_done': 50,\n",
       "  'iterations_done': 31250,\n",
       "  'received_first_batch': True,\n",
       "  'resumed_from': None,\n",
       "  'training_started': True},\n",
       " 'uuid': UUID('56b76c5c-09a2-4116-8300-0711490bbf33')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_log.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_cost</th>\n",
       "      <th>time_initialization</th>\n",
       "      <th>valid_error_rate</th>\n",
       "      <th>train_cost</th>\n",
       "      <th>time_train_this_epoch</th>\n",
       "      <th>time_train_total</th>\n",
       "      <th>time_read_data_this_epoch</th>\n",
       "      <th>time_read_data_total</th>\n",
       "      <th>train_total_gradient_norm</th>\n",
       "      <th>train_error_rate</th>\n",
       "      <th>training_finished</th>\n",
       "      <th>training_finish_requested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [valid_cost, time_initialization, valid_error_rate, train_cost, time_train_this_epoch, time_train_total, time_read_data_this_epoch, time_read_data_total, train_total_gradient_norm, train_error_rate, training_finished, training_finish_requested]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "train_log = loaded_log\n",
    "df = DataFrame.from_dict(train_log,orient='index')\n",
    "\n",
    "index = [cur in train_log.status['_epoch_ends'] for cur in range(len(df))]\n",
    "\n",
    "df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_cost</th>\n",
       "      <th>time_initialization</th>\n",
       "      <th>valid_error_rate</th>\n",
       "      <th>train_cost</th>\n",
       "      <th>time_train_this_epoch</th>\n",
       "      <th>time_train_total</th>\n",
       "      <th>time_read_data_this_epoch</th>\n",
       "      <th>time_read_data_total</th>\n",
       "      <th>train_total_gradient_norm</th>\n",
       "      <th>train_error_rate</th>\n",
       "      <th>training_finished</th>\n",
       "      <th>training_finish_requested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.10271644592</td>\n",
       "      <td>5.416546</td>\n",
       "      <td>0.49920886755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.661934137344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.422468364239</td>\n",
       "      <td>0.679732084274</td>\n",
       "      <td>355.823509</td>\n",
       "      <td>355.823509</td>\n",
       "      <td>62.680616</td>\n",
       "      <td>62.680616</td>\n",
       "      <td>6.95045804977</td>\n",
       "      <td>0.426550030708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>0.642240941525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388844937086</td>\n",
       "      <td>0.651519536972</td>\n",
       "      <td>355.902362</td>\n",
       "      <td>711.725871</td>\n",
       "      <td>64.157025</td>\n",
       "      <td>126.837641</td>\n",
       "      <td>6.38352632523</td>\n",
       "      <td>0.386500030756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>0.626397430897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.37262660265</td>\n",
       "      <td>0.640363097191</td>\n",
       "      <td>356.496310</td>\n",
       "      <td>1068.222181</td>\n",
       "      <td>67.037570</td>\n",
       "      <td>193.875211</td>\n",
       "      <td>6.45698595047</td>\n",
       "      <td>0.371400028467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>0.615903675556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357199370861</td>\n",
       "      <td>0.626807034016</td>\n",
       "      <td>356.744501</td>\n",
       "      <td>1424.966681</td>\n",
       "      <td>69.168647</td>\n",
       "      <td>263.043858</td>\n",
       "      <td>6.86147689819</td>\n",
       "      <td>0.353400021791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>0.608829855919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350474685431</td>\n",
       "      <td>0.616933405399</td>\n",
       "      <td>356.790200</td>\n",
       "      <td>1781.756882</td>\n",
       "      <td>69.452887</td>\n",
       "      <td>332.496745</td>\n",
       "      <td>7.29396820068</td>\n",
       "      <td>0.343350023031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>0.600641191006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333860754967</td>\n",
       "      <td>0.609632372856</td>\n",
       "      <td>356.795067</td>\n",
       "      <td>2138.551948</td>\n",
       "      <td>69.738872</td>\n",
       "      <td>402.235617</td>\n",
       "      <td>7.53929328918</td>\n",
       "      <td>0.338950008154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>0.584486484528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.31329113245</td>\n",
       "      <td>0.600929915905</td>\n",
       "      <td>356.858725</td>\n",
       "      <td>2495.410673</td>\n",
       "      <td>70.748818</td>\n",
       "      <td>472.984435</td>\n",
       "      <td>7.66056442261</td>\n",
       "      <td>0.326700001955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.577991843224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.313686728477</td>\n",
       "      <td>0.593961656094</td>\n",
       "      <td>356.755383</td>\n",
       "      <td>2852.166057</td>\n",
       "      <td>69.299252</td>\n",
       "      <td>542.283686</td>\n",
       "      <td>7.68321371078</td>\n",
       "      <td>0.321100026369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>0.583794951439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.31170886755</td>\n",
       "      <td>0.590962648392</td>\n",
       "      <td>356.803092</td>\n",
       "      <td>3208.969149</td>\n",
       "      <td>69.679551</td>\n",
       "      <td>611.963238</td>\n",
       "      <td>7.69643783569</td>\n",
       "      <td>0.31970000267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6250</th>\n",
       "      <td>0.574492156506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.298655062914</td>\n",
       "      <td>0.586721241474</td>\n",
       "      <td>356.747774</td>\n",
       "      <td>3565.716923</td>\n",
       "      <td>68.676128</td>\n",
       "      <td>680.639366</td>\n",
       "      <td>7.9680018425</td>\n",
       "      <td>0.314700007439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6875</th>\n",
       "      <td>0.566732704639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297863930464</td>\n",
       "      <td>0.580852627754</td>\n",
       "      <td>356.830563</td>\n",
       "      <td>3922.547485</td>\n",
       "      <td>69.461568</td>\n",
       "      <td>750.100934</td>\n",
       "      <td>7.87911367416</td>\n",
       "      <td>0.310450017452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>0.574095129967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.305775314569</td>\n",
       "      <td>0.575468659401</td>\n",
       "      <td>356.705731</td>\n",
       "      <td>4279.253217</td>\n",
       "      <td>68.653818</td>\n",
       "      <td>818.754753</td>\n",
       "      <td>7.95835494995</td>\n",
       "      <td>0.305500000715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>0.572261095047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.305775314569</td>\n",
       "      <td>0.573513805866</td>\n",
       "      <td>356.550159</td>\n",
       "      <td>4635.803375</td>\n",
       "      <td>66.612240</td>\n",
       "      <td>885.366992</td>\n",
       "      <td>8.12510299683</td>\n",
       "      <td>0.302150011063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8750</th>\n",
       "      <td>0.567664802074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301028490067</td>\n",
       "      <td>0.568610131741</td>\n",
       "      <td>356.613693</td>\n",
       "      <td>4992.417069</td>\n",
       "      <td>67.900416</td>\n",
       "      <td>953.267409</td>\n",
       "      <td>8.08692359924</td>\n",
       "      <td>0.296650022268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9375</th>\n",
       "      <td>0.569058418274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301819622517</td>\n",
       "      <td>0.562314033508</td>\n",
       "      <td>356.566965</td>\n",
       "      <td>5348.984034</td>\n",
       "      <td>67.825988</td>\n",
       "      <td>1021.093396</td>\n",
       "      <td>8.29895782471</td>\n",
       "      <td>0.291700005531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.557568907738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285601258278</td>\n",
       "      <td>0.555270314217</td>\n",
       "      <td>356.624631</td>\n",
       "      <td>5705.608665</td>\n",
       "      <td>68.775272</td>\n",
       "      <td>1089.868668</td>\n",
       "      <td>8.39897155762</td>\n",
       "      <td>0.287500023842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10625</th>\n",
       "      <td>0.564020574093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.293512672186</td>\n",
       "      <td>0.555036962032</td>\n",
       "      <td>356.715571</td>\n",
       "      <td>6062.324237</td>\n",
       "      <td>69.867588</td>\n",
       "      <td>1159.736256</td>\n",
       "      <td>8.33026695251</td>\n",
       "      <td>0.286950021982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11250</th>\n",
       "      <td>0.552684247494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.280854433775</td>\n",
       "      <td>0.551729381084</td>\n",
       "      <td>356.627009</td>\n",
       "      <td>6418.951245</td>\n",
       "      <td>68.840951</td>\n",
       "      <td>1228.577208</td>\n",
       "      <td>8.46637630463</td>\n",
       "      <td>0.281200021505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11875</th>\n",
       "      <td>0.544762015343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.277294307947</td>\n",
       "      <td>0.549480199814</td>\n",
       "      <td>356.425877</td>\n",
       "      <td>6775.377122</td>\n",
       "      <td>67.322127</td>\n",
       "      <td>1295.899334</td>\n",
       "      <td>8.53901672363</td>\n",
       "      <td>0.278250008821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>0.535782575607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261471539736</td>\n",
       "      <td>0.545325696468</td>\n",
       "      <td>356.710270</td>\n",
       "      <td>7132.087392</td>\n",
       "      <td>67.834692</td>\n",
       "      <td>1363.734026</td>\n",
       "      <td>8.5252122879</td>\n",
       "      <td>0.280000001192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13125</th>\n",
       "      <td>0.538872599602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.269778490067</td>\n",
       "      <td>0.543784081936</td>\n",
       "      <td>356.450164</td>\n",
       "      <td>7488.537556</td>\n",
       "      <td>78.537441</td>\n",
       "      <td>1442.271467</td>\n",
       "      <td>8.60687828064</td>\n",
       "      <td>0.277050018311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13750</th>\n",
       "      <td>0.528018116951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.255537986755</td>\n",
       "      <td>0.539113998413</td>\n",
       "      <td>356.232941</td>\n",
       "      <td>7844.770497</td>\n",
       "      <td>130.448340</td>\n",
       "      <td>1572.719807</td>\n",
       "      <td>8.70171260834</td>\n",
       "      <td>0.273600012064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>0.540651619434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.276898741722</td>\n",
       "      <td>0.535774350166</td>\n",
       "      <td>356.079904</td>\n",
       "      <td>8200.850400</td>\n",
       "      <td>60.002389</td>\n",
       "      <td>1632.722196</td>\n",
       "      <td>9.00968647003</td>\n",
       "      <td>0.270450025797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>0.522988736629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258306980133</td>\n",
       "      <td>0.532796502113</td>\n",
       "      <td>356.084248</td>\n",
       "      <td>8556.934648</td>\n",
       "      <td>60.333330</td>\n",
       "      <td>1693.055526</td>\n",
       "      <td>8.94982528687</td>\n",
       "      <td>0.267850011587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15625</th>\n",
       "      <td>0.517960309982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25514242053</td>\n",
       "      <td>0.53051751852</td>\n",
       "      <td>355.984042</td>\n",
       "      <td>8912.918691</td>\n",
       "      <td>60.266968</td>\n",
       "      <td>1753.322494</td>\n",
       "      <td>8.89606952667</td>\n",
       "      <td>0.263600021601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16250</th>\n",
       "      <td>0.523187220097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.263053804636</td>\n",
       "      <td>0.526081681252</td>\n",
       "      <td>356.141177</td>\n",
       "      <td>9269.059868</td>\n",
       "      <td>60.971214</td>\n",
       "      <td>1814.293707</td>\n",
       "      <td>9.03664302826</td>\n",
       "      <td>0.264100015163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16875</th>\n",
       "      <td>0.518653452396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254746854305</td>\n",
       "      <td>0.525591790676</td>\n",
       "      <td>356.045099</td>\n",
       "      <td>9625.104967</td>\n",
       "      <td>60.235792</td>\n",
       "      <td>1874.529500</td>\n",
       "      <td>9.02234458923</td>\n",
       "      <td>0.263850003481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>0.52056324482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.252373427153</td>\n",
       "      <td>0.519865334034</td>\n",
       "      <td>356.036319</td>\n",
       "      <td>9981.141287</td>\n",
       "      <td>60.374798</td>\n",
       "      <td>1934.904297</td>\n",
       "      <td>9.13146686554</td>\n",
       "      <td>0.255400002003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18125</th>\n",
       "      <td>0.520134687424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.251582294703</td>\n",
       "      <td>0.517248511314</td>\n",
       "      <td>356.022984</td>\n",
       "      <td>10337.164270</td>\n",
       "      <td>59.910282</td>\n",
       "      <td>1994.814580</td>\n",
       "      <td>9.17405700684</td>\n",
       "      <td>0.257800012827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18750</th>\n",
       "      <td>0.512191712856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.251977860928</td>\n",
       "      <td>0.514928638935</td>\n",
       "      <td>356.074263</td>\n",
       "      <td>10693.238534</td>\n",
       "      <td>60.236780</td>\n",
       "      <td>2055.051360</td>\n",
       "      <td>9.04853916168</td>\n",
       "      <td>0.252700001001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19375</th>\n",
       "      <td>0.503767609596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240506336093</td>\n",
       "      <td>0.508970499039</td>\n",
       "      <td>356.047948</td>\n",
       "      <td>11049.286482</td>\n",
       "      <td>59.755684</td>\n",
       "      <td>2114.807044</td>\n",
       "      <td>9.26647663116</td>\n",
       "      <td>0.250300019979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0.510600149632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.257120251656</td>\n",
       "      <td>0.508899569511</td>\n",
       "      <td>356.125725</td>\n",
       "      <td>11405.412207</td>\n",
       "      <td>59.813610</td>\n",
       "      <td>2174.620654</td>\n",
       "      <td>9.21901035309</td>\n",
       "      <td>0.249500006437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20625</th>\n",
       "      <td>0.502675592899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237737342715</td>\n",
       "      <td>0.505482912064</td>\n",
       "      <td>356.097285</td>\n",
       "      <td>11761.509492</td>\n",
       "      <td>59.749274</td>\n",
       "      <td>2234.369928</td>\n",
       "      <td>9.23189926147</td>\n",
       "      <td>0.248750016093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>0.505979418755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249604433775</td>\n",
       "      <td>0.50201690197</td>\n",
       "      <td>356.181367</td>\n",
       "      <td>12117.690859</td>\n",
       "      <td>60.345965</td>\n",
       "      <td>2294.715893</td>\n",
       "      <td>9.26346492767</td>\n",
       "      <td>0.245300009847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21875</th>\n",
       "      <td>0.498615771532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240901902318</td>\n",
       "      <td>0.500976920128</td>\n",
       "      <td>356.147848</td>\n",
       "      <td>12473.838707</td>\n",
       "      <td>60.028337</td>\n",
       "      <td>2354.744230</td>\n",
       "      <td>9.44353866577</td>\n",
       "      <td>0.242500007153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>0.50072914362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.243670895696</td>\n",
       "      <td>0.497831076384</td>\n",
       "      <td>356.146382</td>\n",
       "      <td>12829.985089</td>\n",
       "      <td>60.082211</td>\n",
       "      <td>2414.826442</td>\n",
       "      <td>9.27966022491</td>\n",
       "      <td>0.239500015974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23125</th>\n",
       "      <td>0.504743933678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.243275329471</td>\n",
       "      <td>0.497113317251</td>\n",
       "      <td>356.185101</td>\n",
       "      <td>13186.170190</td>\n",
       "      <td>61.205303</td>\n",
       "      <td>2476.031745</td>\n",
       "      <td>9.271733284</td>\n",
       "      <td>0.242200016975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23750</th>\n",
       "      <td>0.489391237497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.241693049669</td>\n",
       "      <td>0.494900256395</td>\n",
       "      <td>356.252418</td>\n",
       "      <td>13542.422608</td>\n",
       "      <td>60.128385</td>\n",
       "      <td>2536.160130</td>\n",
       "      <td>9.47745800018</td>\n",
       "      <td>0.238000005484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24375</th>\n",
       "      <td>0.493097692728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.242088615894</td>\n",
       "      <td>0.489736974239</td>\n",
       "      <td>356.195361</td>\n",
       "      <td>13898.617970</td>\n",
       "      <td>60.138498</td>\n",
       "      <td>2596.298628</td>\n",
       "      <td>9.19163990021</td>\n",
       "      <td>0.237550005317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>0.50260078907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.239715188742</td>\n",
       "      <td>0.489410877228</td>\n",
       "      <td>356.198327</td>\n",
       "      <td>14254.816297</td>\n",
       "      <td>60.090522</td>\n",
       "      <td>2656.389149</td>\n",
       "      <td>9.25486183167</td>\n",
       "      <td>0.238550007343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25625</th>\n",
       "      <td>0.487123757601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240110769868</td>\n",
       "      <td>0.482740163803</td>\n",
       "      <td>356.185844</td>\n",
       "      <td>14611.002141</td>\n",
       "      <td>60.344843</td>\n",
       "      <td>2716.733992</td>\n",
       "      <td>9.40135478973</td>\n",
       "      <td>0.229250013828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26250</th>\n",
       "      <td>0.495011895895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.244857594371</td>\n",
       "      <td>0.480933964252</td>\n",
       "      <td>356.239097</td>\n",
       "      <td>14967.241238</td>\n",
       "      <td>60.224773</td>\n",
       "      <td>2776.958765</td>\n",
       "      <td>9.3870306015</td>\n",
       "      <td>0.229600012302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26875</th>\n",
       "      <td>0.487268686295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.244462028146</td>\n",
       "      <td>0.482245385647</td>\n",
       "      <td>356.232801</td>\n",
       "      <td>15323.474039</td>\n",
       "      <td>59.654303</td>\n",
       "      <td>2836.613068</td>\n",
       "      <td>9.39881896973</td>\n",
       "      <td>0.232150018215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27500</th>\n",
       "      <td>0.500246942043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.244066461921</td>\n",
       "      <td>0.477639138699</td>\n",
       "      <td>356.140898</td>\n",
       "      <td>15679.614936</td>\n",
       "      <td>60.130794</td>\n",
       "      <td>2896.743861</td>\n",
       "      <td>9.50002384186</td>\n",
       "      <td>0.228500008583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28125</th>\n",
       "      <td>0.476926058531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223496839404</td>\n",
       "      <td>0.476504266262</td>\n",
       "      <td>356.103687</td>\n",
       "      <td>16035.718623</td>\n",
       "      <td>59.713968</td>\n",
       "      <td>2956.457829</td>\n",
       "      <td>9.46222400665</td>\n",
       "      <td>0.228250011802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28750</th>\n",
       "      <td>0.460574537516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.221123427153</td>\n",
       "      <td>0.472977817059</td>\n",
       "      <td>356.197307</td>\n",
       "      <td>16391.915930</td>\n",
       "      <td>59.837369</td>\n",
       "      <td>3016.295199</td>\n",
       "      <td>9.44227313995</td>\n",
       "      <td>0.227650016546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29375</th>\n",
       "      <td>0.476717889309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233781650662</td>\n",
       "      <td>0.471036195755</td>\n",
       "      <td>356.115450</td>\n",
       "      <td>16748.031379</td>\n",
       "      <td>59.956490</td>\n",
       "      <td>3076.251689</td>\n",
       "      <td>9.47579288483</td>\n",
       "      <td>0.225150004029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>0.454573780298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.221123427153</td>\n",
       "      <td>0.467161595821</td>\n",
       "      <td>356.209452</td>\n",
       "      <td>17104.240832</td>\n",
       "      <td>60.141074</td>\n",
       "      <td>3136.392763</td>\n",
       "      <td>9.55751419067</td>\n",
       "      <td>0.222800016403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30625</th>\n",
       "      <td>0.480449289083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229034811258</td>\n",
       "      <td>0.469035863876</td>\n",
       "      <td>356.144053</td>\n",
       "      <td>17460.384885</td>\n",
       "      <td>60.331349</td>\n",
       "      <td>3196.724112</td>\n",
       "      <td>9.57731723785</td>\n",
       "      <td>0.224650010467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31250</th>\n",
       "      <td>0.470234543085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22468355298</td>\n",
       "      <td>0.463282138109</td>\n",
       "      <td>356.259248</td>\n",
       "      <td>17816.644133</td>\n",
       "      <td>60.707936</td>\n",
       "      <td>3257.432048</td>\n",
       "      <td>9.68754196167</td>\n",
       "      <td>0.220500007272</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           valid_cost  time_initialization valid_error_rate      train_cost  \\\n",
       "0       1.10271644592             5.416546    0.49920886755             NaN   \n",
       "625    0.661934137344                  NaN   0.422468364239  0.679732084274   \n",
       "1250   0.642240941525                  NaN   0.388844937086  0.651519536972   \n",
       "1875   0.626397430897                  NaN    0.37262660265  0.640363097191   \n",
       "2500   0.615903675556                  NaN   0.357199370861  0.626807034016   \n",
       "3125   0.608829855919                  NaN   0.350474685431  0.616933405399   \n",
       "3750   0.600641191006                  NaN   0.333860754967  0.609632372856   \n",
       "4375   0.584486484528                  NaN    0.31329113245  0.600929915905   \n",
       "5000   0.577991843224                  NaN   0.313686728477  0.593961656094   \n",
       "5625   0.583794951439                  NaN    0.31170886755  0.590962648392   \n",
       "6250   0.574492156506                  NaN   0.298655062914  0.586721241474   \n",
       "6875   0.566732704639                  NaN   0.297863930464  0.580852627754   \n",
       "7500   0.574095129967                  NaN   0.305775314569  0.575468659401   \n",
       "8125   0.572261095047                  NaN   0.305775314569  0.573513805866   \n",
       "8750   0.567664802074                  NaN   0.301028490067  0.568610131741   \n",
       "9375   0.569058418274                  NaN   0.301819622517  0.562314033508   \n",
       "10000  0.557568907738                  NaN   0.285601258278  0.555270314217   \n",
       "10625  0.564020574093                  NaN   0.293512672186  0.555036962032   \n",
       "11250  0.552684247494                  NaN   0.280854433775  0.551729381084   \n",
       "11875  0.544762015343                  NaN   0.277294307947  0.549480199814   \n",
       "12500  0.535782575607                  NaN   0.261471539736  0.545325696468   \n",
       "13125  0.538872599602                  NaN   0.269778490067  0.543784081936   \n",
       "13750  0.528018116951                  NaN   0.255537986755  0.539113998413   \n",
       "14375  0.540651619434                  NaN   0.276898741722  0.535774350166   \n",
       "15000  0.522988736629                  NaN   0.258306980133  0.532796502113   \n",
       "15625  0.517960309982                  NaN    0.25514242053   0.53051751852   \n",
       "16250  0.523187220097                  NaN   0.263053804636  0.526081681252   \n",
       "16875  0.518653452396                  NaN   0.254746854305  0.525591790676   \n",
       "17500   0.52056324482                  NaN   0.252373427153  0.519865334034   \n",
       "18125  0.520134687424                  NaN   0.251582294703  0.517248511314   \n",
       "18750  0.512191712856                  NaN   0.251977860928  0.514928638935   \n",
       "19375  0.503767609596                  NaN   0.240506336093  0.508970499039   \n",
       "20000  0.510600149632                  NaN   0.257120251656  0.508899569511   \n",
       "20625  0.502675592899                  NaN   0.237737342715  0.505482912064   \n",
       "21250  0.505979418755                  NaN   0.249604433775   0.50201690197   \n",
       "21875  0.498615771532                  NaN   0.240901902318  0.500976920128   \n",
       "22500   0.50072914362                  NaN   0.243670895696  0.497831076384   \n",
       "23125  0.504743933678                  NaN   0.243275329471  0.497113317251   \n",
       "23750  0.489391237497                  NaN   0.241693049669  0.494900256395   \n",
       "24375  0.493097692728                  NaN   0.242088615894  0.489736974239   \n",
       "25000   0.50260078907                  NaN   0.239715188742  0.489410877228   \n",
       "25625  0.487123757601                  NaN   0.240110769868  0.482740163803   \n",
       "26250  0.495011895895                  NaN   0.244857594371  0.480933964252   \n",
       "26875  0.487268686295                  NaN   0.244462028146  0.482245385647   \n",
       "27500  0.500246942043                  NaN   0.244066461921  0.477639138699   \n",
       "28125  0.476926058531                  NaN   0.223496839404  0.476504266262   \n",
       "28750  0.460574537516                  NaN   0.221123427153  0.472977817059   \n",
       "29375  0.476717889309                  NaN   0.233781650662  0.471036195755   \n",
       "30000  0.454573780298                  NaN   0.221123427153  0.467161595821   \n",
       "30625  0.480449289083                  NaN   0.229034811258  0.469035863876   \n",
       "31250  0.470234543085                  NaN    0.22468355298  0.463282138109   \n",
       "\n",
       "       time_train_this_epoch  time_train_total  time_read_data_this_epoch  \\\n",
       "0                        NaN               NaN                        NaN   \n",
       "625               355.823509        355.823509                  62.680616   \n",
       "1250              355.902362        711.725871                  64.157025   \n",
       "1875              356.496310       1068.222181                  67.037570   \n",
       "2500              356.744501       1424.966681                  69.168647   \n",
       "3125              356.790200       1781.756882                  69.452887   \n",
       "3750              356.795067       2138.551948                  69.738872   \n",
       "4375              356.858725       2495.410673                  70.748818   \n",
       "5000              356.755383       2852.166057                  69.299252   \n",
       "5625              356.803092       3208.969149                  69.679551   \n",
       "6250              356.747774       3565.716923                  68.676128   \n",
       "6875              356.830563       3922.547485                  69.461568   \n",
       "7500              356.705731       4279.253217                  68.653818   \n",
       "8125              356.550159       4635.803375                  66.612240   \n",
       "8750              356.613693       4992.417069                  67.900416   \n",
       "9375              356.566965       5348.984034                  67.825988   \n",
       "10000             356.624631       5705.608665                  68.775272   \n",
       "10625             356.715571       6062.324237                  69.867588   \n",
       "11250             356.627009       6418.951245                  68.840951   \n",
       "11875             356.425877       6775.377122                  67.322127   \n",
       "12500             356.710270       7132.087392                  67.834692   \n",
       "13125             356.450164       7488.537556                  78.537441   \n",
       "13750             356.232941       7844.770497                 130.448340   \n",
       "14375             356.079904       8200.850400                  60.002389   \n",
       "15000             356.084248       8556.934648                  60.333330   \n",
       "15625             355.984042       8912.918691                  60.266968   \n",
       "16250             356.141177       9269.059868                  60.971214   \n",
       "16875             356.045099       9625.104967                  60.235792   \n",
       "17500             356.036319       9981.141287                  60.374798   \n",
       "18125             356.022984      10337.164270                  59.910282   \n",
       "18750             356.074263      10693.238534                  60.236780   \n",
       "19375             356.047948      11049.286482                  59.755684   \n",
       "20000             356.125725      11405.412207                  59.813610   \n",
       "20625             356.097285      11761.509492                  59.749274   \n",
       "21250             356.181367      12117.690859                  60.345965   \n",
       "21875             356.147848      12473.838707                  60.028337   \n",
       "22500             356.146382      12829.985089                  60.082211   \n",
       "23125             356.185101      13186.170190                  61.205303   \n",
       "23750             356.252418      13542.422608                  60.128385   \n",
       "24375             356.195361      13898.617970                  60.138498   \n",
       "25000             356.198327      14254.816297                  60.090522   \n",
       "25625             356.185844      14611.002141                  60.344843   \n",
       "26250             356.239097      14967.241238                  60.224773   \n",
       "26875             356.232801      15323.474039                  59.654303   \n",
       "27500             356.140898      15679.614936                  60.130794   \n",
       "28125             356.103687      16035.718623                  59.713968   \n",
       "28750             356.197307      16391.915930                  59.837369   \n",
       "29375             356.115450      16748.031379                  59.956490   \n",
       "30000             356.209452      17104.240832                  60.141074   \n",
       "30625             356.144053      17460.384885                  60.331349   \n",
       "31250             356.259248      17816.644133                  60.707936   \n",
       "\n",
       "       time_read_data_total train_total_gradient_norm train_error_rate  \\\n",
       "0                       NaN                       NaN              NaN   \n",
       "625               62.680616             6.95045804977   0.426550030708   \n",
       "1250             126.837641             6.38352632523   0.386500030756   \n",
       "1875             193.875211             6.45698595047   0.371400028467   \n",
       "2500             263.043858             6.86147689819   0.353400021791   \n",
       "3125             332.496745             7.29396820068   0.343350023031   \n",
       "3750             402.235617             7.53929328918   0.338950008154   \n",
       "4375             472.984435             7.66056442261   0.326700001955   \n",
       "5000             542.283686             7.68321371078   0.321100026369   \n",
       "5625             611.963238             7.69643783569    0.31970000267   \n",
       "6250             680.639366              7.9680018425   0.314700007439   \n",
       "6875             750.100934             7.87911367416   0.310450017452   \n",
       "7500             818.754753             7.95835494995   0.305500000715   \n",
       "8125             885.366992             8.12510299683   0.302150011063   \n",
       "8750             953.267409             8.08692359924   0.296650022268   \n",
       "9375            1021.093396             8.29895782471   0.291700005531   \n",
       "10000           1089.868668             8.39897155762   0.287500023842   \n",
       "10625           1159.736256             8.33026695251   0.286950021982   \n",
       "11250           1228.577208             8.46637630463   0.281200021505   \n",
       "11875           1295.899334             8.53901672363   0.278250008821   \n",
       "12500           1363.734026              8.5252122879   0.280000001192   \n",
       "13125           1442.271467             8.60687828064   0.277050018311   \n",
       "13750           1572.719807             8.70171260834   0.273600012064   \n",
       "14375           1632.722196             9.00968647003   0.270450025797   \n",
       "15000           1693.055526             8.94982528687   0.267850011587   \n",
       "15625           1753.322494             8.89606952667   0.263600021601   \n",
       "16250           1814.293707             9.03664302826   0.264100015163   \n",
       "16875           1874.529500             9.02234458923   0.263850003481   \n",
       "17500           1934.904297             9.13146686554   0.255400002003   \n",
       "18125           1994.814580             9.17405700684   0.257800012827   \n",
       "18750           2055.051360             9.04853916168   0.252700001001   \n",
       "19375           2114.807044             9.26647663116   0.250300019979   \n",
       "20000           2174.620654             9.21901035309   0.249500006437   \n",
       "20625           2234.369928             9.23189926147   0.248750016093   \n",
       "21250           2294.715893             9.26346492767   0.245300009847   \n",
       "21875           2354.744230             9.44353866577   0.242500007153   \n",
       "22500           2414.826442             9.27966022491   0.239500015974   \n",
       "23125           2476.031745               9.271733284   0.242200016975   \n",
       "23750           2536.160130             9.47745800018   0.238000005484   \n",
       "24375           2596.298628             9.19163990021   0.237550005317   \n",
       "25000           2656.389149             9.25486183167   0.238550007343   \n",
       "25625           2716.733992             9.40135478973   0.229250013828   \n",
       "26250           2776.958765              9.3870306015   0.229600012302   \n",
       "26875           2836.613068             9.39881896973   0.232150018215   \n",
       "27500           2896.743861             9.50002384186   0.228500008583   \n",
       "28125           2956.457829             9.46222400665   0.228250011802   \n",
       "28750           3016.295199             9.44227313995   0.227650016546   \n",
       "29375           3076.251689             9.47579288483   0.225150004029   \n",
       "30000           3136.392763             9.55751419067   0.222800016403   \n",
       "30625           3196.724112             9.57731723785   0.224650010467   \n",
       "31250           3257.432048             9.68754196167   0.220500007272   \n",
       "\n",
       "      training_finished training_finish_requested  \n",
       "0                   NaN                       NaN  \n",
       "625                 NaN                       NaN  \n",
       "1250                NaN                       NaN  \n",
       "1875                NaN                       NaN  \n",
       "2500                NaN                       NaN  \n",
       "3125                NaN                       NaN  \n",
       "3750                NaN                       NaN  \n",
       "4375                NaN                       NaN  \n",
       "5000                NaN                       NaN  \n",
       "5625                NaN                       NaN  \n",
       "6250                NaN                       NaN  \n",
       "6875                NaN                       NaN  \n",
       "7500                NaN                       NaN  \n",
       "8125                NaN                       NaN  \n",
       "8750                NaN                       NaN  \n",
       "9375                NaN                       NaN  \n",
       "10000               NaN                       NaN  \n",
       "10625               NaN                       NaN  \n",
       "11250               NaN                       NaN  \n",
       "11875               NaN                       NaN  \n",
       "12500               NaN                       NaN  \n",
       "13125               NaN                       NaN  \n",
       "13750               NaN                       NaN  \n",
       "14375               NaN                       NaN  \n",
       "15000               NaN                       NaN  \n",
       "15625               NaN                       NaN  \n",
       "16250               NaN                       NaN  \n",
       "16875               NaN                       NaN  \n",
       "17500               NaN                       NaN  \n",
       "18125               NaN                       NaN  \n",
       "18750               NaN                       NaN  \n",
       "19375               NaN                       NaN  \n",
       "20000               NaN                       NaN  \n",
       "20625               NaN                       NaN  \n",
       "21250               NaN                       NaN  \n",
       "21875               NaN                       NaN  \n",
       "22500               NaN                       NaN  \n",
       "23125               NaN                       NaN  \n",
       "23750               NaN                       NaN  \n",
       "24375               NaN                       NaN  \n",
       "25000               NaN                       NaN  \n",
       "25625               NaN                       NaN  \n",
       "26250               NaN                       NaN  \n",
       "26875               NaN                       NaN  \n",
       "27500               NaN                       NaN  \n",
       "28125               NaN                       NaN  \n",
       "28750               NaN                       NaN  \n",
       "29375               NaN                       NaN  \n",
       "30000               NaN                       NaN  \n",
       "30625               NaN                       NaN  \n",
       "31250              True                      True  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['log']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65d64a5c90>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(df['valid_error_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 (CNMeM is enabled with initial size: 80.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "from theano import tensor\n",
    "x = tensor.matrix('features')\n",
    "\n",
    "\n",
    "\n",
    "from blocks.bricks import Linear, Rectifier, Softmax\n",
    "input_to_hidden = Linear(name='input_to_hidden', input_dim=784, output_dim=100)\n",
    "h = Rectifier().apply(input_to_hidden.apply(x))\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=100, output_dim=10)\n",
    "y_hat = Softmax().apply(hidden_to_output.apply(h))\n",
    "\n",
    "y = tensor.lmatrix('targets')\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), y_hat)\n",
    "\n",
    "from blocks.roles import WEIGHT\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.filter import VariableFilter\n",
    "cg = ComputationGraph(cost)\n",
    "W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "cost = cost + 0.005 * (W1 ** 2).sum() + 0.005 * (W2 ** 2).sum()\n",
    "cost.name = 'cost_with_regularization'\n",
    "\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "input_to_hidden.weights_init = hidden_to_output.weights_init = IsotropicGaussian(0.01)\n",
    "input_to_hidden.biases_init = hidden_to_output.biases_init = Constant(0)\n",
    "input_to_hidden.initialize()\n",
    "hidden_to_output.initialize()\n",
    "\n",
    "from fuel.datasets import MNIST\n",
    "mnist = MNIST((\"train\",))\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "data_stream = Flatten(DataStream.default_stream(mnist,\n",
    "                                                iteration_scheme=SequentialScheme(mnist.num_examples, batch_size=256)))\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                             step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "mnist_test = MNIST((\"test\",))\n",
    "data_stream_test = Flatten(DataStream.default_stream(\n",
    "     mnist_test,\n",
    "     iteration_scheme=SequentialScheme(\n",
    "         mnist_test.num_examples, batch_size=1024)))\n",
    "\n",
    "from blocks.log.log import TrainingLog\n",
    "train_log = TrainingLog()\n",
    "\n",
    "path = 'experiment1.tar'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = cg.parameters[0].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks import serialization\n",
    "with open(path,'rb') as src:\n",
    "    parameters = serialization.load_parameters(src)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b, W, b, W]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/input_to_hidden.b',\n",
       " '/hidden_to_output.b',\n",
       " '/input_to_hidden.W',\n",
       " '/hidden_to_output.W']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b, W, b, W]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_produces_examples': False,\n",
       " 'axis_labels': {u'features': (u'batch', u'channel', u'height', u'width'),\n",
       "  u'targets': (u'batch', u'index')},\n",
       " 'data_stream': <fuel.transformers.ScaleAndShift at 0x7f7e5cffdc90>,\n",
       " 'dtype': 'float32',\n",
       " 'iteration_scheme': None,\n",
       " 'which_sources': ('features',)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stream = DataStream.default_stream(mnist,iteration_scheme=SequentialScheme(mnist.num_examples, batch_size=256))\n",
    "data_stream.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = next(data_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9,\n",
       "       1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9,\n",
       "       8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0,\n",
       "       0, 1, 7, 1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7,\n",
       "       4, 6, 8, 0, 7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0,\n",
       "       4, 9, 2, 0, 0, 2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 5, 9, 1, 3, 3, 8,\n",
       "       5, 4, 7, 7, 4, 2, 8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8,\n",
       "       2, 9, 4, 4, 6, 4, 9, 7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9,\n",
       "       1, 7, 6, 2, 8, 2, 2, 5, 0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0,\n",
       "       3, 1, 0, 0, 1, 7, 2, 7, 3, 0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0,\n",
       "       7, 1, 0, 2, 0, 3, 5, 4, 6, 5, 8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2,\n",
       "       2, 3, 3], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_fresh_state': True,\n",
       " 'axis_labels': {u'features': (u'batch', u'channel', u'height', u'width'),\n",
       "  u'targets': (u'batch', u'index')},\n",
       " 'data_state': None,\n",
       " 'dataset': <fuel.datasets.mnist.MNIST at 0x7fbe5c0ed810>,\n",
       " 'iteration_scheme': <fuel.schemes.SequentialScheme at 0x7fbe5c1caad0>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stream = DataStream(mnist,iteration_scheme=SequentialScheme(mnist.num_examples, batch_size=256))\n",
    "data_stream.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.config import config\n",
    "config.profile = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set config.profile = True to show a more detailed log after call to main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.log.log import TrainingLog\n",
    "train_log = TrainingLog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'experiment1.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "train_monitor = TrainingDataMonitoring([cost],after_batch=True,prefix=\"train\")\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring\n",
    "test_monitor = DataStreamMonitoring(\n",
    "    variables=[cost], data_stream=data_stream_test, prefix=\"test\")\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing, Timing, ProgressBar\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks_extras.extensions.plot import Plot \n",
    "from blocks.model import Model\n",
    "model = Model(cost)\n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm, log=train_log, model=model,\n",
    "                     extensions=[test_monitor, train_monitor,\n",
    "                                 ProgressBar(),Timing(),FinishAfter(every_n_epochs=5),\n",
    "                                 Checkpoint(path,save_separately=['log'],save_main_loop=False), Printing()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "train_monitor = TrainingDataMonitoring([cost],after_batch=True,prefix=\"train\")\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring\n",
    "test_monitor = DataStreamMonitoring(\n",
    "    variables=[cost], data_stream=data_stream_test, prefix=\"test\")\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing, Timing, ProgressBar\n",
    "from blocks.extensions.saveload import Checkpoint, Load\n",
    "from blocks_extras.extensions.plot import Plot \n",
    "from blocks.model import Model\n",
    "model = Model(cost)\n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm, model=model,\n",
    "                     extensions=[test_monitor, train_monitor,\n",
    "                                 ProgressBar(),Timing(),Load(path,load_log=True),\n",
    "                                 Checkpoint(path,save_separately=['log'],save_main_loop=False),\n",
    "                                 Printing(),FinishAfter(every_n_epochs=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 235 |                                      | Elapsed Time: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t test_cost_with_regularization: 2.34190654755\n",
      "\t time_initialization: 0.699547052383\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, step 235 |                                      | Elapsed Time: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 235\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 235:\n",
      "\t test_cost_with_regularization: 0.657896518707\n",
      "\t time_read_data_this_epoch: 0.243763923645\n",
      "\t time_read_data_total: 0.243763923645\n",
      "\t time_train_this_epoch: 0.189972639084\n",
      "\t time_train_total: 0.189972639084\n",
      "\t train_cost_with_regularization: 0.717392504215\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2, step 235 |                                      | Elapsed Time: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 470\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 470:\n",
      "\t test_cost_with_regularization: 0.572150051594\n",
      "\t time_read_data_this_epoch: 0.237758874893\n",
      "\t time_read_data_total: 0.481522798538\n",
      "\t time_train_this_epoch: 0.187118053436\n",
      "\t time_train_total: 0.37709069252\n",
      "\t train_cost_with_regularization: 0.624714553356\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3, step 235 |                                      | Elapsed Time: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 705\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 705:\n",
      "\t test_cost_with_regularization: 0.54877436161\n",
      "\t time_read_data_this_epoch: 0.240235328674\n",
      "\t time_read_data_total: 0.721758127213\n",
      "\t time_train_this_epoch: 0.1893658638\n",
      "\t time_train_total: 0.56645655632\n",
      "\t train_cost_with_regularization: 0.598186254501\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4, step 235 |                                      | Elapsed Time: 0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 940\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 940:\n",
      "\t test_cost_with_regularization: 0.537450730801\n",
      "\t time_read_data_this_epoch: 0.238004207611\n",
      "\t time_read_data_total: 0.959762334824\n",
      "\t time_train_this_epoch: 0.187265396118\n",
      "\t time_train_total: 0.753721952438\n",
      "\t train_cost_with_regularization: 0.586212694645\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 1175\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1175:\n",
      "\t test_cost_with_regularization: 0.530255973339\n",
      "\t time_read_data_this_epoch: 0.235489368439\n",
      "\t time_read_data_total: 1.19525170326\n",
      "\t time_train_this_epoch: 0.186561346054\n",
      "\t time_train_total: 0.940283298492\n",
      "\t train_cost_with_regularization: 0.578282177448\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 1175\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1175:\n",
      "\t saved_to: ('experiment1.tar',)\n",
      "\t test_cost_with_regularization: 0.530255973339\n",
      "\t time_read_data_this_epoch: 0.235489368439\n",
      "\t time_read_data_total: 1.19525170326\n",
      "\t time_train_this_epoch: 0.186561346054\n",
      "\t time_train_total: 0.940283298492\n",
      "\t train_cost_with_regularization: 0.578282177448\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from blocks import serialization\n",
    "\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"filename '_pkl' not found\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7c686ee50cb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mserialization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'experiment1.tar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/serialization.pyc\u001b[0m in \u001b[0;36mcontinue_training\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mchange_recursion_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursion_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mmain_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m     \u001b[0mmain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file_, name, use_cpickle)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtar_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         p = unpickler(\n\u001b[1;32m--> 258\u001b[1;33m             tar_file.extractfile(tar_file.getmember(name)))\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'_parameters'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtar_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_PersistentLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtar_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/tarfile.pyc\u001b[0m in \u001b[0;36mgetmember\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1819\u001b[0m         \u001b[0mtarinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getmember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1821\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filename %r not found\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1822\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"filename '_pkl' not found\""
     ]
    }
   ],
   "source": [
    "serialization.continue_training('experiment1.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<blocks.extensions.monitoring.DataStreamMonitoring at 0x7f7e65704e50>,\n",
       " <blocks.extensions.monitoring.TrainingDataMonitoring at 0x7f7e65719490>,\n",
       " <blocks.extensions.ProgressBar at 0x7f7e65719450>,\n",
       " <blocks.extensions.Timing at 0x7f7e5d703110>,\n",
       " <blocks.extensions.saveload.Load at 0x7f7e5d703790>,\n",
       " <blocks.extensions.saveload.Checkpoint at 0x7f7e5d703b50>,\n",
       " <blocks.extensions.Printing at 0x7f7e656a8e50>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_loop.extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.tensor.sharedvar.TensorSharedVariable"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(main_loop.algorithm.step_rule.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'seek'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e71e55ce1854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserialization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file_, name, use_cpickle)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[1;32m--> 251\u001b[1;33m     \u001b[0mfile_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# To be able to read several objects in one file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_cpickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'seek'"
     ]
    }
   ],
   "source": [
    "o = serialization.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'blocks' from '/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/__init__.pyc'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import blocks\n",
    "reload(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj = open(path,'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"filename '_pkl' not found\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-514be3be347e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mserialization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/serialization.pyc\u001b[0m in \u001b[0;36mcontinue_training\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mchange_recursion_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursion_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mmain_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m     \u001b[0mmain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file_, name, use_cpickle)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtar_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         p = unpickler(\n\u001b[1;32m--> 258\u001b[1;33m             tar_file.extractfile(tar_file.getmember(name)))\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'_parameters'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtar_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_PersistentLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtar_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/tarfile.pyc\u001b[0m in \u001b[0;36mgetmember\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1819\u001b[0m         \u001b[0mtarinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getmember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1821\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filename %r not found\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1822\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"filename '_pkl' not found\""
     ]
    }
   ],
   "source": [
    "serialization.continue_training(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6eaa5b0e0faf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mblocks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmainloop_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserialization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file_, name, use_cpickle)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'_parameters'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtar_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_PersistentLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtar_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_reduce\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m         \u001b[0mstack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m_constructor_Function\u001b[1;34m(maker, input_storage, inputs_data)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\u001b[0m(1028)\u001b[0;36m_constructor_Function\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   1026 \u001b[1;33m        \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1027 \u001b[1;33m            \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m-> 1028 \u001b[1;33m            \u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1029 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1030 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> container.data.dtype\n",
      "'float32'\n",
      "ipdb> type(container.data)\n",
      "<type 'CudaNdarray'>\n",
      "ipdb> type(x)\n",
      "<type 'CudaNdarray'>\n",
      "ipdb> container.data.shape\n",
      "(784, 100)\n",
      "ipdb> x.shape\n",
      "(784, 100)\n",
      "ipdb> container.data==x\n",
      "False\n",
      "ipdb> isinstance(x, numpy.ndarray)\n",
      "False\n",
      "ipdb> (container.data == x).all())\n",
      "*** SyntaxError: invalid syntax (<stdin>, line 1)\n",
      "ipdb> container.data is x\n",
      "False\n",
      "ipdb> (container.data == x).all()\n",
      "*** AttributeError: 'bool' object has no attribute 'all'\n",
      "ipdb> isinstance(x, numpy.ndarray)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from blocks import serialization\n",
    "with open(path,'rb') as src:\n",
    "    mainloop_loaded = serialization.load(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/input_to_hidden.b',\n",
       " '/hidden_to_output.b',\n",
       " '/input_to_hidden.W',\n",
       " '/hidden_to_output.W']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tarball = tarfile.open('experiment1.tar','r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = numpy.load(tarball.extractfile(tarball.getmember('_parameters')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|hidden_to_output.W',\n",
       " '|hidden_to_output.b',\n",
       " '|input_to_hidden.W',\n",
       " '|input_to_hidden.b']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_parameters', '_pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarball.getnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tarball.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7f2668cf4a5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain_loop_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '_'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(path,'rb') as src:\n",
    "    main_loop_loaded = pickle.load(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.saveload import Load \n",
    "out = Load(\"7_dogvcats.pkl\",load_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.extensions.saveload import Load \n",
    "out = Load(\"mnist.pkl\",load_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'load_iteration_state': False,\n",
       " 'load_log': True,\n",
       " 'name': 'Load',\n",
       " 'path': 'mnist.pkl'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_produces_examples': False,\n",
       " 'axis_labels': {u'features': (u'batch', 'feature'),\n",
       "  u'targets': (u'batch', 'feature')},\n",
       " 'child_epoch_iterator': <fuel.iterator.DataIterator at 0x7fd152d5ed10>,\n",
       " 'data_stream': <fuel.transformers.Cast at 0x7fd1563c4fd0>,\n",
       " 'iteration_scheme': None,\n",
       " 'which_sources': (u'features', u'targets')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_loop.iteration_state[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_log = main_loop_loaded.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_log = main_loop.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': {'_epoch_ends': [235, 470, 705, 940, 1175],\n",
       "  'batch_interrupt_received': False,\n",
       "  'epoch_interrupt_received': False,\n",
       "  'epoch_started': False,\n",
       "  'epochs_done': 5,\n",
       "  'iterations_done': 1175,\n",
       "  'received_first_batch': True,\n",
       "  'resumed_from': None,\n",
       "  'training_started': True},\n",
       " 'uuid': UUID('d7932814-3f49-4815-ba12-0c08762d4705')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log.status['_epoch_ends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_initialization</th>\n",
       "      <th>test_cost_with_regularization</th>\n",
       "      <th>train_cost_with_regularization</th>\n",
       "      <th>time_train_this_epoch</th>\n",
       "      <th>time_train_total</th>\n",
       "      <th>time_read_data_this_epoch</th>\n",
       "      <th>time_read_data_total</th>\n",
       "      <th>got_exception</th>\n",
       "      <th>loaded_from</th>\n",
       "      <th>training_finished</th>\n",
       "      <th>training_finish_requested</th>\n",
       "      <th>saved_to</th>\n",
       "      <th>epoch_interrupt_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657896399498</td>\n",
       "      <td>0.71739256382</td>\n",
       "      <td>0.819406</td>\n",
       "      <td>0.819406</td>\n",
       "      <td>0.710626</td>\n",
       "      <td>0.710626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572149515152</td>\n",
       "      <td>0.624711632729</td>\n",
       "      <td>0.810645</td>\n",
       "      <td>1.630050</td>\n",
       "      <td>0.713767</td>\n",
       "      <td>1.424393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.548774182796</td>\n",
       "      <td>0.598183274269</td>\n",
       "      <td>0.813459</td>\n",
       "      <td>2.443510</td>\n",
       "      <td>0.694995</td>\n",
       "      <td>2.119389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537450432777</td>\n",
       "      <td>0.58619761467</td>\n",
       "      <td>0.823344</td>\n",
       "      <td>3.266854</td>\n",
       "      <td>0.697063</td>\n",
       "      <td>2.816452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530254781246</td>\n",
       "      <td>0.578256845474</td>\n",
       "      <td>0.823769</td>\n",
       "      <td>4.090623</td>\n",
       "      <td>0.697535</td>\n",
       "      <td>3.513987</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>experiment1.pkl</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.524686157703</td>\n",
       "      <td>0.572647929192</td>\n",
       "      <td>0.657924</td>\n",
       "      <td>0.657924</td>\n",
       "      <td>0.745206</td>\n",
       "      <td>0.745206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520373106003</td>\n",
       "      <td>0.568329393864</td>\n",
       "      <td>0.657227</td>\n",
       "      <td>1.315151</td>\n",
       "      <td>0.733772</td>\n",
       "      <td>1.478978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516869544983</td>\n",
       "      <td>0.565397083759</td>\n",
       "      <td>0.658486</td>\n",
       "      <td>1.973637</td>\n",
       "      <td>0.735127</td>\n",
       "      <td>2.214105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.513993144035</td>\n",
       "      <td>0.562805771828</td>\n",
       "      <td>0.658683</td>\n",
       "      <td>2.632320</td>\n",
       "      <td>0.735785</td>\n",
       "      <td>2.949889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.51168859005</td>\n",
       "      <td>0.560897111893</td>\n",
       "      <td>0.659297</td>\n",
       "      <td>3.291616</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>3.683703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>(experiment1.pkl,)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509774506092</td>\n",
       "      <td>0.559388637543</td>\n",
       "      <td>0.636934</td>\n",
       "      <td>3.928550</td>\n",
       "      <td>0.688237</td>\n",
       "      <td>4.371940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508123278618</td>\n",
       "      <td>0.558055102825</td>\n",
       "      <td>0.637285</td>\n",
       "      <td>4.565835</td>\n",
       "      <td>0.688063</td>\n",
       "      <td>5.060003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.506580591202</td>\n",
       "      <td>0.556847035885</td>\n",
       "      <td>0.638733</td>\n",
       "      <td>5.204568</td>\n",
       "      <td>0.689317</td>\n",
       "      <td>5.749320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505239605904</td>\n",
       "      <td>0.556078314781</td>\n",
       "      <td>0.638510</td>\n",
       "      <td>5.843078</td>\n",
       "      <td>0.689652</td>\n",
       "      <td>6.438972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.503964781761</td>\n",
       "      <td>0.555191397667</td>\n",
       "      <td>0.638581</td>\n",
       "      <td>6.481659</td>\n",
       "      <td>0.686292</td>\n",
       "      <td>7.125264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>experiment1.pkl</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.502898812294</td>\n",
       "      <td>0.554610490799</td>\n",
       "      <td>0.826503</td>\n",
       "      <td>0.826503</td>\n",
       "      <td>0.695102</td>\n",
       "      <td>0.695102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5019967556</td>\n",
       "      <td>0.554097235203</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>1.641674</td>\n",
       "      <td>0.684489</td>\n",
       "      <td>1.379591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501249730587</td>\n",
       "      <td>0.553524076939</td>\n",
       "      <td>0.813871</td>\n",
       "      <td>2.455545</td>\n",
       "      <td>0.682863</td>\n",
       "      <td>2.062454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500490069389</td>\n",
       "      <td>0.552942097187</td>\n",
       "      <td>0.814043</td>\n",
       "      <td>3.269588</td>\n",
       "      <td>0.680591</td>\n",
       "      <td>2.743045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.499844968319</td>\n",
       "      <td>0.552291452885</td>\n",
       "      <td>0.815052</td>\n",
       "      <td>4.084640</td>\n",
       "      <td>0.684071</td>\n",
       "      <td>3.427116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>experiment1.pkl</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4935</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.499250113964</td>\n",
       "      <td>0.551552593708</td>\n",
       "      <td>0.649809</td>\n",
       "      <td>0.649809</td>\n",
       "      <td>0.672696</td>\n",
       "      <td>0.672696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.498757749796</td>\n",
       "      <td>0.550807476044</td>\n",
       "      <td>0.651115</td>\n",
       "      <td>1.300924</td>\n",
       "      <td>0.671702</td>\n",
       "      <td>1.344397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.498271793127</td>\n",
       "      <td>0.549954533577</td>\n",
       "      <td>0.647011</td>\n",
       "      <td>1.947935</td>\n",
       "      <td>0.669205</td>\n",
       "      <td>2.013602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497827142477</td>\n",
       "      <td>0.549173116684</td>\n",
       "      <td>0.647747</td>\n",
       "      <td>2.595682</td>\n",
       "      <td>0.677682</td>\n",
       "      <td>2.691284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497404396534</td>\n",
       "      <td>0.548478186131</td>\n",
       "      <td>0.647975</td>\n",
       "      <td>3.243657</td>\n",
       "      <td>0.680176</td>\n",
       "      <td>3.371461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497037708759</td>\n",
       "      <td>0.547836780548</td>\n",
       "      <td>0.648415</td>\n",
       "      <td>3.892073</td>\n",
       "      <td>0.681535</td>\n",
       "      <td>4.052996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496713578701</td>\n",
       "      <td>0.547131478786</td>\n",
       "      <td>0.648176</td>\n",
       "      <td>4.540249</td>\n",
       "      <td>0.680396</td>\n",
       "      <td>4.733392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6580</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49640339613</td>\n",
       "      <td>0.546566545963</td>\n",
       "      <td>0.648109</td>\n",
       "      <td>5.188357</td>\n",
       "      <td>0.680553</td>\n",
       "      <td>5.413945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496075212955</td>\n",
       "      <td>0.54603523016</td>\n",
       "      <td>0.648161</td>\n",
       "      <td>5.836518</td>\n",
       "      <td>0.680089</td>\n",
       "      <td>6.094034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495806366205</td>\n",
       "      <td>0.545576691628</td>\n",
       "      <td>0.648105</td>\n",
       "      <td>6.484623</td>\n",
       "      <td>0.680497</td>\n",
       "      <td>6.774531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>experiment1.pkl</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495521456003</td>\n",
       "      <td>0.545186758041</td>\n",
       "      <td>0.653094</td>\n",
       "      <td>0.653094</td>\n",
       "      <td>0.668964</td>\n",
       "      <td>0.668964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495275557041</td>\n",
       "      <td>0.544660210609</td>\n",
       "      <td>0.653352</td>\n",
       "      <td>1.306446</td>\n",
       "      <td>0.669078</td>\n",
       "      <td>1.338042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7755</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495034039021</td>\n",
       "      <td>0.54445707798</td>\n",
       "      <td>0.652027</td>\n",
       "      <td>1.958473</td>\n",
       "      <td>0.669356</td>\n",
       "      <td>2.007398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494829177856</td>\n",
       "      <td>0.544138550758</td>\n",
       "      <td>0.651730</td>\n",
       "      <td>2.610203</td>\n",
       "      <td>0.669320</td>\n",
       "      <td>2.676718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8225</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49461466074</td>\n",
       "      <td>0.543837070465</td>\n",
       "      <td>0.652615</td>\n",
       "      <td>3.262818</td>\n",
       "      <td>0.669777</td>\n",
       "      <td>3.346496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494388669729</td>\n",
       "      <td>0.543370485306</td>\n",
       "      <td>0.651480</td>\n",
       "      <td>3.914299</td>\n",
       "      <td>0.671035</td>\n",
       "      <td>4.017530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49419260025</td>\n",
       "      <td>0.54311722517</td>\n",
       "      <td>0.651935</td>\n",
       "      <td>4.566234</td>\n",
       "      <td>0.673031</td>\n",
       "      <td>4.690561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8930</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493984460831</td>\n",
       "      <td>0.542944848537</td>\n",
       "      <td>0.652558</td>\n",
       "      <td>5.218792</td>\n",
       "      <td>0.672868</td>\n",
       "      <td>5.363429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9165</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493812471628</td>\n",
       "      <td>0.542757511139</td>\n",
       "      <td>0.652805</td>\n",
       "      <td>5.871597</td>\n",
       "      <td>0.671661</td>\n",
       "      <td>6.035089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493612229824</td>\n",
       "      <td>0.542616963387</td>\n",
       "      <td>0.652571</td>\n",
       "      <td>6.524168</td>\n",
       "      <td>0.670796</td>\n",
       "      <td>6.705886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9635</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493452310562</td>\n",
       "      <td>0.542409002781</td>\n",
       "      <td>0.652359</td>\n",
       "      <td>7.176527</td>\n",
       "      <td>0.669811</td>\n",
       "      <td>7.375697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493306070566</td>\n",
       "      <td>0.542329668999</td>\n",
       "      <td>0.652228</td>\n",
       "      <td>7.828755</td>\n",
       "      <td>0.671227</td>\n",
       "      <td>8.046924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10105</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493157923222</td>\n",
       "      <td>0.5422077775</td>\n",
       "      <td>0.652304</td>\n",
       "      <td>8.481059</td>\n",
       "      <td>0.670510</td>\n",
       "      <td>8.717434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.493046820164</td>\n",
       "      <td>0.541953206062</td>\n",
       "      <td>0.652617</td>\n",
       "      <td>9.133676</td>\n",
       "      <td>0.670552</td>\n",
       "      <td>9.387986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10575</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492912381887</td>\n",
       "      <td>0.541863560677</td>\n",
       "      <td>0.652269</td>\n",
       "      <td>9.785945</td>\n",
       "      <td>0.668872</td>\n",
       "      <td>10.056858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10810</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492802798748</td>\n",
       "      <td>0.541778087616</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>10.438119</td>\n",
       "      <td>0.669835</td>\n",
       "      <td>10.726693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11045</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492696136236</td>\n",
       "      <td>0.54155766964</td>\n",
       "      <td>0.652858</td>\n",
       "      <td>11.090977</td>\n",
       "      <td>0.672090</td>\n",
       "      <td>11.398782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11280</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492571413517</td>\n",
       "      <td>0.541499495506</td>\n",
       "      <td>0.653026</td>\n",
       "      <td>11.744004</td>\n",
       "      <td>0.672463</td>\n",
       "      <td>12.071245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492452770472</td>\n",
       "      <td>0.541370391846</td>\n",
       "      <td>0.652863</td>\n",
       "      <td>12.396867</td>\n",
       "      <td>0.671551</td>\n",
       "      <td>12.742796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11750</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492339372635</td>\n",
       "      <td>0.541390120983</td>\n",
       "      <td>0.652537</td>\n",
       "      <td>13.049404</td>\n",
       "      <td>0.672350</td>\n",
       "      <td>13.415146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11985</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49222868681</td>\n",
       "      <td>0.541271448135</td>\n",
       "      <td>0.657040</td>\n",
       "      <td>13.706444</td>\n",
       "      <td>0.677949</td>\n",
       "      <td>14.093095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12220</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492115825415</td>\n",
       "      <td>0.541204869747</td>\n",
       "      <td>0.663540</td>\n",
       "      <td>14.369984</td>\n",
       "      <td>0.687474</td>\n",
       "      <td>14.780570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49200296402</td>\n",
       "      <td>0.541304826736</td>\n",
       "      <td>0.662645</td>\n",
       "      <td>15.032629</td>\n",
       "      <td>0.687680</td>\n",
       "      <td>15.468249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12690</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.491905033588</td>\n",
       "      <td>0.541272819042</td>\n",
       "      <td>0.652506</td>\n",
       "      <td>15.685135</td>\n",
       "      <td>0.671067</td>\n",
       "      <td>16.139317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.491833865643</td>\n",
       "      <td>0.541314303875</td>\n",
       "      <td>0.651820</td>\n",
       "      <td>16.336955</td>\n",
       "      <td>0.669381</td>\n",
       "      <td>16.808698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13160</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.491724163294</td>\n",
       "      <td>0.541306257248</td>\n",
       "      <td>0.651698</td>\n",
       "      <td>16.988653</td>\n",
       "      <td>0.667354</td>\n",
       "      <td>17.476052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(experiment1.pkl,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_initialization test_cost_with_regularization  \\\n",
       "235                    NaN                0.657896399498   \n",
       "470                    NaN                0.572149515152   \n",
       "705                    NaN                0.548774182796   \n",
       "940                    NaN                0.537450432777   \n",
       "1175                   NaN                0.530254781246   \n",
       "1410                   NaN                0.524686157703   \n",
       "1645                   NaN                0.520373106003   \n",
       "1880                   NaN                0.516869544983   \n",
       "2115                   NaN                0.513993144035   \n",
       "2350                   NaN                 0.51168859005   \n",
       "2585                   NaN                0.509774506092   \n",
       "2820                   NaN                0.508123278618   \n",
       "3055                   NaN                0.506580591202   \n",
       "3290                   NaN                0.505239605904   \n",
       "3525                   NaN                0.503964781761   \n",
       "3760                   NaN                0.502898812294   \n",
       "3995                   NaN                  0.5019967556   \n",
       "4230                   NaN                0.501249730587   \n",
       "4465                   NaN                0.500490069389   \n",
       "4700                   NaN                0.499844968319   \n",
       "4935                   NaN                0.499250113964   \n",
       "5170                   NaN                0.498757749796   \n",
       "5405                   NaN                0.498271793127   \n",
       "5640                   NaN                0.497827142477   \n",
       "5875                   NaN                0.497404396534   \n",
       "6110                   NaN                0.497037708759   \n",
       "6345                   NaN                0.496713578701   \n",
       "6580                   NaN                 0.49640339613   \n",
       "6815                   NaN                0.496075212955   \n",
       "7050                   NaN                0.495806366205   \n",
       "7285                   NaN                0.495521456003   \n",
       "7520                   NaN                0.495275557041   \n",
       "7755                   NaN                0.495034039021   \n",
       "7990                   NaN                0.494829177856   \n",
       "8225                   NaN                 0.49461466074   \n",
       "8460                   NaN                0.494388669729   \n",
       "8695                   NaN                 0.49419260025   \n",
       "8930                   NaN                0.493984460831   \n",
       "9165                   NaN                0.493812471628   \n",
       "9400                   NaN                0.493612229824   \n",
       "9635                   NaN                0.493452310562   \n",
       "9870                   NaN                0.493306070566   \n",
       "10105                  NaN                0.493157923222   \n",
       "10340                  NaN                0.493046820164   \n",
       "10575                  NaN                0.492912381887   \n",
       "10810                  NaN                0.492802798748   \n",
       "11045                  NaN                0.492696136236   \n",
       "11280                  NaN                0.492571413517   \n",
       "11515                  NaN                0.492452770472   \n",
       "11750                  NaN                0.492339372635   \n",
       "11985                  NaN                 0.49222868681   \n",
       "12220                  NaN                0.492115825415   \n",
       "12455                  NaN                 0.49200296402   \n",
       "12690                  NaN                0.491905033588   \n",
       "12925                  NaN                0.491833865643   \n",
       "13160                  NaN                0.491724163294   \n",
       "\n",
       "      train_cost_with_regularization  time_train_this_epoch  time_train_total  \\\n",
       "235                    0.71739256382               0.819406          0.819406   \n",
       "470                   0.624711632729               0.810645          1.630050   \n",
       "705                   0.598183274269               0.813459          2.443510   \n",
       "940                    0.58619761467               0.823344          3.266854   \n",
       "1175                  0.578256845474               0.823769          4.090623   \n",
       "1410                  0.572647929192               0.657924          0.657924   \n",
       "1645                  0.568329393864               0.657227          1.315151   \n",
       "1880                  0.565397083759               0.658486          1.973637   \n",
       "2115                  0.562805771828               0.658683          2.632320   \n",
       "2350                  0.560897111893               0.659297          3.291616   \n",
       "2585                  0.559388637543               0.636934          3.928550   \n",
       "2820                  0.558055102825               0.637285          4.565835   \n",
       "3055                  0.556847035885               0.638733          5.204568   \n",
       "3290                  0.556078314781               0.638510          5.843078   \n",
       "3525                  0.555191397667               0.638581          6.481659   \n",
       "3760                  0.554610490799               0.826503          0.826503   \n",
       "3995                  0.554097235203               0.815171          1.641674   \n",
       "4230                  0.553524076939               0.813871          2.455545   \n",
       "4465                  0.552942097187               0.814043          3.269588   \n",
       "4700                  0.552291452885               0.815052          4.084640   \n",
       "4935                  0.551552593708               0.649809          0.649809   \n",
       "5170                  0.550807476044               0.651115          1.300924   \n",
       "5405                  0.549954533577               0.647011          1.947935   \n",
       "5640                  0.549173116684               0.647747          2.595682   \n",
       "5875                  0.548478186131               0.647975          3.243657   \n",
       "6110                  0.547836780548               0.648415          3.892073   \n",
       "6345                  0.547131478786               0.648176          4.540249   \n",
       "6580                  0.546566545963               0.648109          5.188357   \n",
       "6815                   0.54603523016               0.648161          5.836518   \n",
       "7050                  0.545576691628               0.648105          6.484623   \n",
       "7285                  0.545186758041               0.653094          0.653094   \n",
       "7520                  0.544660210609               0.653352          1.306446   \n",
       "7755                   0.54445707798               0.652027          1.958473   \n",
       "7990                  0.544138550758               0.651730          2.610203   \n",
       "8225                  0.543837070465               0.652615          3.262818   \n",
       "8460                  0.543370485306               0.651480          3.914299   \n",
       "8695                   0.54311722517               0.651935          4.566234   \n",
       "8930                  0.542944848537               0.652558          5.218792   \n",
       "9165                  0.542757511139               0.652805          5.871597   \n",
       "9400                  0.542616963387               0.652571          6.524168   \n",
       "9635                  0.542409002781               0.652359          7.176527   \n",
       "9870                  0.542329668999               0.652228          7.828755   \n",
       "10105                   0.5422077775               0.652304          8.481059   \n",
       "10340                 0.541953206062               0.652617          9.133676   \n",
       "10575                 0.541863560677               0.652269          9.785945   \n",
       "10810                 0.541778087616               0.652174         10.438119   \n",
       "11045                  0.54155766964               0.652858         11.090977   \n",
       "11280                 0.541499495506               0.653026         11.744004   \n",
       "11515                 0.541370391846               0.652863         12.396867   \n",
       "11750                 0.541390120983               0.652537         13.049404   \n",
       "11985                 0.541271448135               0.657040         13.706444   \n",
       "12220                 0.541204869747               0.663540         14.369984   \n",
       "12455                 0.541304826736               0.662645         15.032629   \n",
       "12690                 0.541272819042               0.652506         15.685135   \n",
       "12925                 0.541314303875               0.651820         16.336955   \n",
       "13160                 0.541306257248               0.651698         16.988653   \n",
       "\n",
       "       time_read_data_this_epoch  time_read_data_total  \\\n",
       "235                     0.710626              0.710626   \n",
       "470                     0.713767              1.424393   \n",
       "705                     0.694995              2.119389   \n",
       "940                     0.697063              2.816452   \n",
       "1175                    0.697535              3.513987   \n",
       "1410                    0.745206              0.745206   \n",
       "1645                    0.733772              1.478978   \n",
       "1880                    0.735127              2.214105   \n",
       "2115                    0.735785              2.949889   \n",
       "2350                    0.733813              3.683703   \n",
       "2585                    0.688237              4.371940   \n",
       "2820                    0.688063              5.060003   \n",
       "3055                    0.689317              5.749320   \n",
       "3290                    0.689652              6.438972   \n",
       "3525                    0.686292              7.125264   \n",
       "3760                    0.695102              0.695102   \n",
       "3995                    0.684489              1.379591   \n",
       "4230                    0.682863              2.062454   \n",
       "4465                    0.680591              2.743045   \n",
       "4700                    0.684071              3.427116   \n",
       "4935                    0.672696              0.672696   \n",
       "5170                    0.671702              1.344397   \n",
       "5405                    0.669205              2.013602   \n",
       "5640                    0.677682              2.691284   \n",
       "5875                    0.680176              3.371461   \n",
       "6110                    0.681535              4.052996   \n",
       "6345                    0.680396              4.733392   \n",
       "6580                    0.680553              5.413945   \n",
       "6815                    0.680089              6.094034   \n",
       "7050                    0.680497              6.774531   \n",
       "7285                    0.668964              0.668964   \n",
       "7520                    0.669078              1.338042   \n",
       "7755                    0.669356              2.007398   \n",
       "7990                    0.669320              2.676718   \n",
       "8225                    0.669777              3.346496   \n",
       "8460                    0.671035              4.017530   \n",
       "8695                    0.673031              4.690561   \n",
       "8930                    0.672868              5.363429   \n",
       "9165                    0.671661              6.035089   \n",
       "9400                    0.670796              6.705886   \n",
       "9635                    0.669811              7.375697   \n",
       "9870                    0.671227              8.046924   \n",
       "10105                   0.670510              8.717434   \n",
       "10340                   0.670552              9.387986   \n",
       "10575                   0.668872             10.056858   \n",
       "10810                   0.669835             10.726693   \n",
       "11045                   0.672090             11.398782   \n",
       "11280                   0.672463             12.071245   \n",
       "11515                   0.671551             12.742796   \n",
       "11750                   0.672350             13.415146   \n",
       "11985                   0.677949             14.093095   \n",
       "12220                   0.687474             14.780570   \n",
       "12455                   0.687680             15.468249   \n",
       "12690                   0.671067             16.139317   \n",
       "12925                   0.669381             16.808698   \n",
       "13160                   0.667354             17.476052   \n",
       "\n",
       "                                           got_exception      loaded_from  \\\n",
       "235                                                  NaN              NaN   \n",
       "470                                                  NaN              NaN   \n",
       "705                                                  NaN              NaN   \n",
       "940                                                  NaN              NaN   \n",
       "1175   Traceback (most recent call last):\\n  File \"/h...  experiment1.pkl   \n",
       "1410                                                 NaN              NaN   \n",
       "1645                                                 NaN              NaN   \n",
       "1880                                                 NaN              NaN   \n",
       "2115                                                 NaN              NaN   \n",
       "2350                                                 NaN              NaN   \n",
       "2585                                                 NaN              NaN   \n",
       "2820                                                 NaN              NaN   \n",
       "3055                                                 NaN              NaN   \n",
       "3290                                                 NaN              NaN   \n",
       "3525                                                 NaN  experiment1.pkl   \n",
       "3760                                                 NaN              NaN   \n",
       "3995                                                 NaN              NaN   \n",
       "4230                                                 NaN              NaN   \n",
       "4465                                                 NaN              NaN   \n",
       "4700                                                 NaN  experiment1.pkl   \n",
       "4935                                                 NaN              NaN   \n",
       "5170                                                 NaN              NaN   \n",
       "5405                                                 NaN              NaN   \n",
       "5640                                                 NaN              NaN   \n",
       "5875                                                 NaN              NaN   \n",
       "6110                                                 NaN              NaN   \n",
       "6345                                                 NaN              NaN   \n",
       "6580                                                 NaN              NaN   \n",
       "6815                                                 NaN              NaN   \n",
       "7050                                                 NaN  experiment1.pkl   \n",
       "7285                                                 NaN              NaN   \n",
       "7520                                                 NaN              NaN   \n",
       "7755                                                 NaN              NaN   \n",
       "7990                                                 NaN              NaN   \n",
       "8225                                                 NaN              NaN   \n",
       "8460                                                 NaN              NaN   \n",
       "8695                                                 NaN              NaN   \n",
       "8930                                                 NaN              NaN   \n",
       "9165                                                 NaN              NaN   \n",
       "9400                                                 NaN              NaN   \n",
       "9635                                                 NaN              NaN   \n",
       "9870                                                 NaN              NaN   \n",
       "10105                                                NaN              NaN   \n",
       "10340                                                NaN              NaN   \n",
       "10575                                                NaN              NaN   \n",
       "10810                                                NaN              NaN   \n",
       "11045                                                NaN              NaN   \n",
       "11280                                                NaN              NaN   \n",
       "11515                                                NaN              NaN   \n",
       "11750                                                NaN              NaN   \n",
       "11985                                                NaN              NaN   \n",
       "12220                                                NaN              NaN   \n",
       "12455                                                NaN              NaN   \n",
       "12690                                                NaN              NaN   \n",
       "12925                                                NaN              NaN   \n",
       "13160                                                NaN              NaN   \n",
       "\n",
       "      training_finished training_finish_requested            saved_to  \\\n",
       "235                 NaN                       NaN                 NaN   \n",
       "470                 NaN                       NaN                 NaN   \n",
       "705                 NaN                       NaN                 NaN   \n",
       "940                 NaN                       NaN                 NaN   \n",
       "1175               True                      True                 NaN   \n",
       "1410                NaN                       NaN                 NaN   \n",
       "1645                NaN                       NaN                 NaN   \n",
       "1880                NaN                       NaN                 NaN   \n",
       "2115                NaN                       NaN                 NaN   \n",
       "2350               True                      True  (experiment1.pkl,)   \n",
       "2585                NaN                       NaN                 NaN   \n",
       "2820                NaN                       NaN                 NaN   \n",
       "3055                NaN                       NaN                 NaN   \n",
       "3290                NaN                       NaN                 NaN   \n",
       "3525               True                      True                 NaN   \n",
       "3760                NaN                       NaN                 NaN   \n",
       "3995                NaN                       NaN                 NaN   \n",
       "4230                NaN                       NaN                 NaN   \n",
       "4465                NaN                       NaN                 NaN   \n",
       "4700               True                      True                 NaN   \n",
       "4935                NaN                       NaN                 NaN   \n",
       "5170                NaN                       NaN                 NaN   \n",
       "5405                NaN                       NaN                 NaN   \n",
       "5640                NaN                       NaN                 NaN   \n",
       "5875                NaN                       NaN                 NaN   \n",
       "6110                NaN                       NaN                 NaN   \n",
       "6345                NaN                       NaN                 NaN   \n",
       "6580                NaN                       NaN                 NaN   \n",
       "6815                NaN                       NaN                 NaN   \n",
       "7050               True                      True                 NaN   \n",
       "7285                NaN                       NaN                 NaN   \n",
       "7520                NaN                       NaN                 NaN   \n",
       "7755                NaN                       NaN                 NaN   \n",
       "7990                NaN                       NaN                 NaN   \n",
       "8225                NaN                       NaN                 NaN   \n",
       "8460                NaN                       NaN                 NaN   \n",
       "8695                NaN                       NaN                 NaN   \n",
       "8930                NaN                       NaN                 NaN   \n",
       "9165                NaN                       NaN                 NaN   \n",
       "9400                NaN                       NaN                 NaN   \n",
       "9635                NaN                       NaN                 NaN   \n",
       "9870                NaN                       NaN                 NaN   \n",
       "10105               NaN                       NaN                 NaN   \n",
       "10340               NaN                       NaN                 NaN   \n",
       "10575               NaN                       NaN                 NaN   \n",
       "10810               NaN                       NaN                 NaN   \n",
       "11045               NaN                       NaN                 NaN   \n",
       "11280               NaN                       NaN                 NaN   \n",
       "11515               NaN                       NaN                 NaN   \n",
       "11750               NaN                       NaN                 NaN   \n",
       "11985               NaN                       NaN                 NaN   \n",
       "12220               NaN                       NaN                 NaN   \n",
       "12455               NaN                       NaN                 NaN   \n",
       "12690               NaN                       NaN                 NaN   \n",
       "12925               NaN                       NaN                 NaN   \n",
       "13160              True                       NaN  (experiment1.pkl,)   \n",
       "\n",
       "      epoch_interrupt_received  \n",
       "235                        NaN  \n",
       "470                        NaN  \n",
       "705                        NaN  \n",
       "940                        NaN  \n",
       "1175                       NaN  \n",
       "1410                       NaN  \n",
       "1645                       NaN  \n",
       "1880                       NaN  \n",
       "2115                       NaN  \n",
       "2350                       NaN  \n",
       "2585                       NaN  \n",
       "2820                       NaN  \n",
       "3055                       NaN  \n",
       "3290                       NaN  \n",
       "3525                       NaN  \n",
       "3760                       NaN  \n",
       "3995                       NaN  \n",
       "4230                       NaN  \n",
       "4465                       NaN  \n",
       "4700                       NaN  \n",
       "4935                       NaN  \n",
       "5170                       NaN  \n",
       "5405                       NaN  \n",
       "5640                       NaN  \n",
       "5875                       NaN  \n",
       "6110                       NaN  \n",
       "6345                       NaN  \n",
       "6580                       NaN  \n",
       "6815                       NaN  \n",
       "7050                       NaN  \n",
       "7285                       NaN  \n",
       "7520                       NaN  \n",
       "7755                       NaN  \n",
       "7990                       NaN  \n",
       "8225                       NaN  \n",
       "8460                       NaN  \n",
       "8695                       NaN  \n",
       "8930                       NaN  \n",
       "9165                       NaN  \n",
       "9400                       NaN  \n",
       "9635                       NaN  \n",
       "9870                       NaN  \n",
       "10105                      NaN  \n",
       "10340                      NaN  \n",
       "10575                      NaN  \n",
       "10810                      NaN  \n",
       "11045                      NaN  \n",
       "11280                      NaN  \n",
       "11515                      NaN  \n",
       "11750                      NaN  \n",
       "11985                      NaN  \n",
       "12220                      NaN  \n",
       "12455                      NaN  \n",
       "12690                      NaN  \n",
       "12925                      NaN  \n",
       "13160                     True  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame.from_dict(train_log,orient='index')\n",
    "\n",
    "index = [cur in train_log.status['_epoch_ends'] for cur in range(len(df))]\n",
    "\n",
    "df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_initialization</th>\n",
       "      <th>test_cost_with_regularization</th>\n",
       "      <th>train_cost_with_regularization</th>\n",
       "      <th>time_train_this_epoch</th>\n",
       "      <th>time_train_total</th>\n",
       "      <th>time_read_data_this_epoch</th>\n",
       "      <th>time_read_data_total</th>\n",
       "      <th>training_finished</th>\n",
       "      <th>training_finish_requested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657896399498</td>\n",
       "      <td>0.71739256382</td>\n",
       "      <td>0.819406</td>\n",
       "      <td>0.819406</td>\n",
       "      <td>0.710626</td>\n",
       "      <td>0.710626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572149515152</td>\n",
       "      <td>0.624711632729</td>\n",
       "      <td>0.810645</td>\n",
       "      <td>1.630050</td>\n",
       "      <td>0.713767</td>\n",
       "      <td>1.424393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.548774182796</td>\n",
       "      <td>0.598183274269</td>\n",
       "      <td>0.813459</td>\n",
       "      <td>2.443510</td>\n",
       "      <td>0.694995</td>\n",
       "      <td>2.119389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537450432777</td>\n",
       "      <td>0.58619761467</td>\n",
       "      <td>0.823344</td>\n",
       "      <td>3.266854</td>\n",
       "      <td>0.697063</td>\n",
       "      <td>2.816452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530254781246</td>\n",
       "      <td>0.578256845474</td>\n",
       "      <td>0.823769</td>\n",
       "      <td>4.090623</td>\n",
       "      <td>0.697535</td>\n",
       "      <td>3.513987</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_initialization test_cost_with_regularization  \\\n",
       "235                   NaN                0.657896399498   \n",
       "470                   NaN                0.572149515152   \n",
       "705                   NaN                0.548774182796   \n",
       "940                   NaN                0.537450432777   \n",
       "1175                  NaN                0.530254781246   \n",
       "\n",
       "     train_cost_with_regularization  time_train_this_epoch  time_train_total  \\\n",
       "235                   0.71739256382               0.819406          0.819406   \n",
       "470                  0.624711632729               0.810645          1.630050   \n",
       "705                  0.598183274269               0.813459          2.443510   \n",
       "940                   0.58619761467               0.823344          3.266854   \n",
       "1175                 0.578256845474               0.823769          4.090623   \n",
       "\n",
       "      time_read_data_this_epoch  time_read_data_total training_finished  \\\n",
       "235                    0.710626              0.710626               NaN   \n",
       "470                    0.713767              1.424393               NaN   \n",
       "705                    0.694995              2.119389               NaN   \n",
       "940                    0.697063              2.816452               NaN   \n",
       "1175                   0.697535              3.513987              True   \n",
       "\n",
       "     training_finish_requested  \n",
       "235                        NaN  \n",
       "470                        NaN  \n",
       "705                        NaN  \n",
       "940                        NaN  \n",
       "1175                      True  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame.from_dict(train_log,orient='index')\n",
    "\n",
    "index = [cur in train_log.status['_epoch_ends'] for cur in range(len(df))]\n",
    "\n",
    "df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_cost_with_regularization': array(0.5302547812461853, dtype=float32),\n",
       " 'time_read_data_this_epoch': 0.6926867961883545,\n",
       " 'time_read_data_total': 3.5557937622070312,\n",
       " 'time_train_this_epoch': 0.689683198928833,\n",
       " 'time_train_total': 3.986610174179077,\n",
       " 'train_cost_with_regularization': array(0.5782568454742432, dtype=float32),\n",
       " 'training_finish_requested': True,\n",
       " 'training_finished': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log[1175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_loop = MainLoop(data_stream = data_stream, algorithm=algorithm,\n",
    "                     extensions=[FinishAfter(every_n_epochs=20),\n",
    "                                 TrainingDataMonitoring([cost],after_batch=True),\n",
    "                                 Plot('Plotting example',channels=[['cost_with_regularization']],\n",
    "                                      after_batch=True, open_browser=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n"
     ]
    }
   ],
   "source": [
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm,\n",
    "                     extensions=[monitor, FinishAfter(after_n_epochs=5), Printing(),\n",
    "                                Plot('Plotting example', channels=[['cost_with_regularization']],\n",
    "                      after_batch=True, open_browser=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN RESUMED\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 1175\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 1175:\n",
      "\t test_cost_with_regularization: 0.529210686684\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 6\n",
      "\t iterations_done: 1410\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 1410:\n",
      "\t test_cost_with_regularization: 0.523926854134\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 7\n",
      "\t iterations_done: 1645\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 1645:\n",
      "\t test_cost_with_regularization: 0.5198341012\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 8\n",
      "\t iterations_done: 1880\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 1880:\n",
      "\t test_cost_with_regularization: 0.516628026962\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 9\n",
      "\t iterations_done: 2115\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 2115:\n",
      "\t test_cost_with_regularization: 0.513839125633\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 2350\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 2350:\n",
      "\t test_cost_with_regularization: 0.511581122875\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 11\n",
      "\t iterations_done: 2585\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 2585:\n",
      "\t test_cost_with_regularization: 0.509766101837\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 12\n",
      "\t iterations_done: 2820\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 2820:\n",
      "\t test_cost_with_regularization: 0.508126437664\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 13\n",
      "\t iterations_done: 3055\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 3055:\n",
      "\t test_cost_with_regularization: 0.506747484207\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 14\n",
      "\t iterations_done: 3290\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 3290:\n",
      "\t test_cost_with_regularization: 0.505525588989\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 15\n",
      "\t iterations_done: 3525\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 3525:\n",
      "\t test_cost_with_regularization: 0.50443726778\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 16\n",
      "\t iterations_done: 3760\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 3760:\n",
      "\t test_cost_with_regularization: 0.503444254398\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 17\n",
      "\t iterations_done: 3995\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 3995:\n",
      "\t test_cost_with_regularization: 0.50256717205\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 18\n",
      "\t iterations_done: 4230\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 4230:\n",
      "\t test_cost_with_regularization: 0.501814126968\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 19\n",
      "\t iterations_done: 4465\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 4465:\n",
      "\t test_cost_with_regularization: 0.501042306423\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 20\n",
      "\t iterations_done: 4700\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 4700:\n",
      "\t test_cost_with_regularization: 0.500346302986\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 21\n",
      "\t iterations_done: 4935\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 4935:\n",
      "\t test_cost_with_regularization: 0.499723821878\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 22\n",
      "\t iterations_done: 5170\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 5170:\n",
      "\t test_cost_with_regularization: 0.499165385962\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 23\n",
      "\t iterations_done: 5405\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 5405:\n",
      "\t test_cost_with_regularization: 0.498638391495\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 24\n",
      "\t iterations_done: 5640\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 5640:\n",
      "\t test_cost_with_regularization: 0.498099148273\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 25\n",
      "\t iterations_done: 5875\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 5875:\n",
      "\t test_cost_with_regularization: 0.497612386942\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 26\n",
      "\t iterations_done: 6110\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 6110:\n",
      "\t test_cost_with_regularization: 0.497166216373\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 27\n",
      "\t iterations_done: 6345\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 6345:\n",
      "\t test_cost_with_regularization: 0.496781408787\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 28\n",
      "\t iterations_done: 6580\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 6580:\n",
      "\t test_cost_with_regularization: 0.496384859085\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 29\n",
      "\t iterations_done: 6815\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 6815:\n",
      "\t test_cost_with_regularization: 0.495969206095\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 30\n",
      "\t iterations_done: 7050\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 7050:\n",
      "\t test_cost_with_regularization: 0.495644181967\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 31\n",
      "\t iterations_done: 7285\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 7285:\n",
      "\t test_cost_with_regularization: 0.495319455862\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 32\n",
      "\t iterations_done: 7520\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 7520:\n",
      "\t test_cost_with_regularization: 0.495047658682\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 33\n",
      "\t iterations_done: 7755\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 7755:\n",
      "\t test_cost_with_regularization: 0.494755655527\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 34\n",
      "\t iterations_done: 7990\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 7990:\n",
      "\t test_cost_with_regularization: 0.494490474463\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 35\n",
      "\t iterations_done: 8225\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 8225:\n",
      "\t test_cost_with_regularization: 0.494265943766\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 36\n",
      "\t iterations_done: 8460\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 8460:\n",
      "\t test_cost_with_regularization: 0.494016587734\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 37\n",
      "\t iterations_done: 8695\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 8695:\n",
      "\t test_cost_with_regularization: 0.49383482337\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 38\n",
      "\t iterations_done: 8930\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 8930:\n",
      "\t test_cost_with_regularization: 0.493587583303\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 39\n",
      "\t iterations_done: 9165\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 9165:\n",
      "\t test_cost_with_regularization: 0.493350923061\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 40\n",
      "\t iterations_done: 9400\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 9400:\n",
      "\t test_cost_with_regularization: 0.493213474751\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 41\n",
      "\t iterations_done: 9635\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 9635:\n",
      "\t test_cost_with_regularization: 0.49305576086\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 42\n",
      "\t iterations_done: 9870\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 9870:\n",
      "\t test_cost_with_regularization: 0.492814719677\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 43\n",
      "\t iterations_done: 10105\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 10105:\n",
      "\t test_cost_with_regularization: 0.492639541626\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 44\n",
      "\t iterations_done: 10340\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 10340:\n",
      "\t test_cost_with_regularization: 0.492447435856\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 45\n",
      "\t iterations_done: 10575\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 10575:\n",
      "\t test_cost_with_regularization: 0.492291837931\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 46\n",
      "\t iterations_done: 10810\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 10810:\n",
      "\t test_cost_with_regularization: 0.492127656937\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 47\n",
      "\t iterations_done: 11045\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 11045:\n",
      "\t test_cost_with_regularization: 0.491969525814\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 48\n",
      "\t iterations_done: 11280\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 11280:\n",
      "\t test_cost_with_regularization: 0.491807073355\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 49\n",
      "\t iterations_done: 11515\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 11515:\n",
      "\t test_cost_with_regularization: 0.491700738668\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 50\n",
      "\t iterations_done: 11750\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 11750:\n",
      "\t test_cost_with_regularization: 0.491556555033\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 51\n",
      "\t iterations_done: 11985\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 11985:\n",
      "\t test_cost_with_regularization: 0.491410672665\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 52\n",
      "\t iterations_done: 12220\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 12220:\n",
      "\t test_cost_with_regularization: 0.491341888905\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 53\n",
      "\t iterations_done: 12455\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 12455:\n",
      "\t test_cost_with_regularization: 0.491220712662\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 54\n",
      "\t iterations_done: 12690\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 12690:\n",
      "\t test_cost_with_regularization: 0.491144359112\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 55\n",
      "\t iterations_done: 12925\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 12925:\n",
      "\t test_cost_with_regularization: 0.491055905819\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 56\n",
      "\t iterations_done: 13160\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 13160:\n",
      "\t test_cost_with_regularization: 0.490974515676\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 57\n",
      "\t iterations_done: 13395\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 13395:\n",
      "\t test_cost_with_regularization: 0.490882307291\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 58\n",
      "\t iterations_done: 13630\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 13630:\n",
      "\t test_cost_with_regularization: 0.49081403017\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 59\n",
      "\t iterations_done: 13865\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 13865:\n",
      "\t test_cost_with_regularization: 0.490710556507\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 60\n",
      "\t iterations_done: 14100\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 14100:\n",
      "\t test_cost_with_regularization: 0.49068275094\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 61\n",
      "\t iterations_done: 14335\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 14335:\n",
      "\t test_cost_with_regularization: 0.490569353104\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 62\n",
      "\t iterations_done: 14570\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 14570:\n",
      "\t test_cost_with_regularization: 0.490556150675\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 63\n",
      "\t iterations_done: 14805\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 14805:\n",
      "\t test_cost_with_regularization: 0.490488678217\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 64\n",
      "\t iterations_done: 15040\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 15040:\n",
      "\t test_cost_with_regularization: 0.490447700024\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 65\n",
      "\t iterations_done: 15275\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 15275:\n",
      "\t test_cost_with_regularization: 0.490389674902\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 66\n",
      "\t iterations_done: 15510\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 15510:\n",
      "\t test_cost_with_regularization: 0.490323841572\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 67\n",
      "\t iterations_done: 15745\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 15745:\n",
      "\t test_cost_with_regularization: 0.49025401473\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 68\n",
      "\t iterations_done: 15980\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 15980:\n",
      "\t test_cost_with_regularization: 0.490188419819\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 69\n",
      "\t iterations_done: 16215\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 16215:\n",
      "\t test_cost_with_regularization: 0.490165859461\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 70\n",
      "\t iterations_done: 16450\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 16450:\n",
      "\t test_cost_with_regularization: 0.490172564983\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 71\n",
      "\t iterations_done: 16685\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 16685:\n",
      "\t test_cost_with_regularization: 0.490112960339\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 72\n",
      "\t iterations_done: 16920\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 16920:\n",
      "\t test_cost_with_regularization: 0.490130841732\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 73\n",
      "\t iterations_done: 17155\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 17155:\n",
      "\t test_cost_with_regularization: 0.490079313517\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:blocks.main_loop:Received epoch interrupt signal.\n",
      "\n",
      "Blocks will complete this epoch of training and run extensions before exiting. If you do not want to complete this epoch, press CTRL + C again to stop training after the current batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 74\n",
      "\t iterations_done: 17390\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 17390:\n",
      "\t test_cost_with_regularization: 0.490055799484\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 75\n",
      "\t iterations_done: 17625\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 17625:\n",
      "\t test_cost_with_regularization: 0.490035861731\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: True\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 75\n",
      "\t iterations_done: 17625\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: c59c1ee084ba44358b438db30b130278\n",
      "\t training_started: True\n",
      "Log records from the iteration 17625:\n",
      "\t test_cost_with_regularization: 0.490035861731\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh server code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "a = theano.shared(3.)\n",
    "a.name = 'a'\n",
    "x = theano.tensor.scalar('data')\n",
    "cost = abs(x ** 2 - x ** a)\n",
    "cost.name = 'cost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from fuel.streams import DataStream\n",
    "from fuel.datasets import IterableDataset\n",
    "data_stream = DataStream(IterableDataset(\n",
    "    numpy.random.rand(500).astype(theano.config.floatX)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterator = data_stream.get_epoch_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.62995422,  0.32033223,  0.68399209,  0.20138139,  0.74530929,\n",
       "         0.09568401,  0.85822088,  0.97524863,  0.38963047,  0.7030682 ,\n",
       "         0.05060682,  0.56913966,  0.54841715,  0.43221915,  0.61322057,\n",
       "         0.3587175 ,  0.12958211,  0.18302345,  0.83260292,  0.31777167,\n",
       "         0.12428147,  0.79015273,  0.81031448,  0.10155449,  0.49074605,\n",
       "         0.14298512,  0.84532189,  0.39250526,  0.58600199,  0.26728666,\n",
       "         0.97566158,  0.52819735,  0.29919446,  0.03327228,  0.42022645,\n",
       "         0.65690374,  0.42308095,  0.41687271,  0.80983174,  0.80132806,\n",
       "         0.37195206,  0.80272681,  0.19894855,  0.42496505,  0.47498664,\n",
       "         0.19455802,  0.57906926,  0.80857152,  0.84696859,  0.67938221,\n",
       "         0.68840683,  0.7554667 ,  0.18831593,  0.60918337,  0.92633879,\n",
       "         0.04145744,  0.36704054,  0.976309  ,  0.32266459,  0.99555773,\n",
       "         0.67181897,  0.7331211 ,  0.75046718,  0.52418256,  0.80947912,\n",
       "         0.38059583,  0.40812591,  0.81051606,  0.94128758,  0.56298596,\n",
       "         0.55637956,  0.99238157,  0.02925476,  0.43867859,  0.99232656,\n",
       "         0.90892476,  0.50208777,  0.25938737,  0.7277717 ,  0.55030799,\n",
       "         0.53993452,  0.76575553,  0.08527505,  0.20346665,  0.01374771,\n",
       "         0.96886969,  0.02103052,  0.87679142,  0.6982215 ,  0.53546262,\n",
       "         0.74850959,  0.86936408,  0.45499161,  0.63181102,  0.90989548,\n",
       "         0.35220829,  0.09977113,  0.67913795,  0.48797393,  0.4499211 ], dtype=float32),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n"
     ]
    }
   ],
   "source": [
    "from blocks.main_loop import MainLoop\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import FinishAfter\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks_extras.extensions.plot import Plot \n",
    "main_loop = MainLoop(\n",
    "     model=None, data_stream=data_stream,\n",
    "     algorithm=GradientDescent(cost=cost,\n",
    "                               parameters=[a],\n",
    "                               step_rule=Scale(learning_rate=0.1)),\n",
    "     extensions=[FinishAfter(after_n_epochs=1),\n",
    "                 TrainingDataMonitoring([cost, a], after_batch=True),\n",
    "                 Plot('Plotting example', channels=[['cost'], ['a']],\n",
    "                      after_batch=True,open_browser=False)])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:/home/yiulau/anaconda2/lib/python2.7/site-packages/bokeh/validation/check.pyc:W-1001 (NO_GLYPH_RENDERERS): Plot has no glyph renderers: Figure, ViewModel:Plot, ref _id: 1e3dd265-657d-4e65-a955-20d8b1b4e503\n",
      "ERROR:/home/yiulau/anaconda2/lib/python2.7/site-packages/bokeh/validation/check.pyc:W-1001 (NO_GLYPH_RENDERERS): Plot has no glyph renderers: Figure, ViewModel:Plot, ref _id: a51d0ece-c5ef-4f69-9452-e7652394024a\n"
     ]
    }
   ],
   "source": [
    "main_loop.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks_extras.extensions.plot import Plot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import blocks_extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print_shape: Shape.0\n"
     ]
    }
   ],
   "source": [
    "from blocks.utils import print_shape\n",
    "print_shape(hidden_to_output._parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "a = theano.shared(3.)\n",
    "a.name = 'a'\n",
    "x = theano.tensor.vector('data')\n",
    "cost = abs(x ** 2 - x ** a).mean()\n",
    "cost.name = 'cost'\n",
    "from blocks.graph import ComputationGraph\n",
    "cg = ComputationGraph(cost)\n",
    "from blocks.roles import add_role, PARAMETER\n",
    "add_role(a,PARAMETER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.schemes import SequentialScheme\n",
    "scheme = SequentialScheme(examples=500,batch_size=20)\n",
    "import numpy\n",
    "from fuel.streams import DataStream\n",
    "from fuel.datasets import IndexableDataset\n",
    "data_stream = DataStream(IndexableDataset(\n",
    "    numpy.random.rand(500,).astype(theano.config.floatX)),iteration_scheme=scheme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.main_loop import MainLoop\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.extensions import FinishAfter\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks_extras.extensions.plot import Plot \n",
    "path2 = 'experiment2.pkl'\n",
    "from blocks.extensions.saveload import Checkpoint, Load\n",
    "from blocks.model import Model\n",
    "model = Model(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n"
     ]
    }
   ],
   "source": [
    "main_loop = MainLoop(\n",
    "     model=model, data_stream=data_stream,\n",
    "     algorithm=GradientDescent(cost=cost,\n",
    "                               parameters=[a],\n",
    "                               step_rule=Scale(learning_rate=0.1)),\n",
    "     extensions=[TrainingDataMonitoring([cost, a], after_batch=True),\n",
    "                 Plot('Plotting example', channels=[['cost'], ['a']],\n",
    "                      after_batch=True,open_browser=False),FinishAfter(every_n_epochs=3),Checkpoint(path2)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_loop = MainLoop(\n",
    "     model=model, data_stream=data_stream,\n",
    "     algorithm=GradientDescent(cost=cost,\n",
    "                               parameters=[a],\n",
    "                               step_rule=Scale(learning_rate=0.1)),\n",
    "     extensions=[TrainingDataMonitoring([cost, a], after_batch=True),\n",
    "                 Plot('Plotting example', channels=[['cost'], ['a']],\n",
    "                      after_batch=True,open_browser=False),\n",
    "                 FinishAfter(every_n_epochs=3),Checkpoint(path2),load(path2,load_log=True)])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:/home/yiulau/anaconda2/lib/python2.7/site-packages/bokeh/validation/check.pyc:W-1001 (NO_GLYPH_RENDERERS): Plot has no glyph renderers: Figure, ViewModel:Plot, ref _id: 1797bdca-f017-4085-83e8-24f578330165\n",
      "ERROR:/home/yiulau/anaconda2/lib/python2.7/site-packages/bokeh/validation/check.pyc:W-1001 (NO_GLYPH_RENDERERS): Plot has no glyph renderers: Figure, ViewModel:Plot, ref _id: 3892b3c9-9127-433e-bf05-850ceb8d928c\n"
     ]
    }
   ],
   "source": [
    "main_loop.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2.003629315813212)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = next(data_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95684004,  0.96635509,  0.1149674 ,  0.0621931 ,  0.56958795,\n",
       "        0.20774239,  0.62010759,  0.44613704,  0.21505079,  0.7664777 ,\n",
       "        0.31065831,  0.7282989 ,  0.72993624,  0.58612084,  0.85176593,\n",
       "        0.4146165 ,  0.24861267,  0.11099321,  0.1064222 ,  0.5013513 ,\n",
       "        0.13522606,  0.15946144,  0.66585457,  0.27781764,  0.44307238,\n",
       "        0.76801133,  0.25866547,  0.46829286,  0.46792322,  0.61924142,\n",
       "        0.69755805,  0.99244899,  0.09844369,  0.69932061,  0.0242563 ,\n",
       "        0.97599053,  0.27689633,  0.2284074 ,  0.74800438,  0.18814416,\n",
       "        0.5347333 ,  0.83301991,  0.95778781,  0.35415092,  0.97332573,\n",
       "        0.16325518,  0.42033148,  0.57371485,  0.60563415,  0.39394981,\n",
       "        0.85944784,  0.66941082,  0.53340024,  0.92420429,  0.462859  ,\n",
       "        0.17955385,  0.78538185,  0.07971132,  0.3680968 ,  0.16186741,\n",
       "        0.59746546,  0.67275071,  0.94267178,  0.16763142,  0.74919665,\n",
       "        0.94227159,  0.13963354,  0.90059   ,  0.05125659,  0.26999444,\n",
       "        0.97570413,  0.70447332,  0.47459546,  0.56660682,  0.33021146,\n",
       "        0.19537091,  0.79493314,  0.42182139,  0.03367504,  0.60190725,\n",
       "        0.32021561,  0.85796171,  0.38352495,  0.7968775 ,  0.39863709,\n",
       "        0.60577118,  0.20883466,  0.87291366,  0.75354475,  0.87680924,\n",
       "        0.58994883,  0.06281373,  0.33113411,  0.96153015,  0.86159641,\n",
       "        0.54714608,  0.1431649 ,  0.1436619 ,  0.55667335,  0.78832173], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.08255316224045732)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.eval({x:batch[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if from_server:\n",
    "    train_stream = ServerDataStream(('image_features','target'),False,port=5007)\n",
    "    valid_stream = ServerDataStream(('image_features','target'),False,port=5008)\n",
    "else:\n",
    "    train_stream = get_datastream((128,128),1.5,256,128,'train',(128,128))\n",
    "    valid_stream = get_datastream((128,128),1.5,256,128,'train',(128,128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--batch-size'], dest='batch_size', nargs=None, const=None, default=500, type=<type 'int'>, choices=None, help='Batch size.', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: An example of training a convolutional network on the MNIST dataset.\n",
      "       [-h] [--num-epochs NUM_EPOCHS]\n",
      "       [--feature-maps FEATURE_MAPS [FEATURE_MAPS ...]]\n",
      "       [--mlp-hiddens MLP_HIDDENS [MLP_HIDDENS ...]]\n",
      "       [--conv-sizes CONV_SIZES [CONV_SIZES ...]]\n",
      "       [--pool-sizes POOL_SIZES [POOL_SIZES ...]] [--batch-size BATCH_SIZE]\n",
      "       [save_to]\n",
      "An example of training a convolutional network on the MNIST dataset.: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: An example of training a convolutional network on the MNIST dataset.\n",
      "       [-h] [--num-epochs NUM_EPOCHS]\n",
      "       [--feature-maps FEATURE_MAPS [FEATURE_MAPS ...]]\n",
      "       [--mlp-hiddens MLP_HIDDENS [MLP_HIDDENS ...]]\n",
      "       [--conv-sizes CONV_SIZES [CONV_SIZES ...]]\n",
      "       [--pool-sizes POOL_SIZES [POOL_SIZES ...]] [--batch-size BATCH_SIZE]\n",
      "       [save_to]\n",
      "An example of training a convolutional network on the MNIST dataset.: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-6af4bfdf5e16>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-6af4bfdf5e16>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    **vars(args)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 (CNMeM is enabled with initial size: 50.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.dogs_vs_cats import DogsVsCats\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.transformers.image import RandomFixedSizeCrop\n",
    "from fuel.transformers import Flatten, ScaleAndShift, Cast\n",
    "from fuel.server import start_server\n",
    "from rescale_transformer import rescale_transformer\n",
    "\n",
    "def get_datastream(random_cropsize,max_height_width_ratio,\n",
    "             minimum_dim_len,batchsize,require_stream,rescale_outputsize=None):\n",
    "    \"\"\" Returns a datastream with all the necessary transforms applied.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    random_cropsize : tuple\n",
    "        The size of the random crop as applied by RandomFixedSizeCrop\n",
    "    max_heigh_width_ratio : float\n",
    "        See rescale_transformer.\n",
    "    minimum_dim_len : int\n",
    "        See rescale_transformer.\n",
    "    batchsize : int\n",
    "        Batch size to be the minibatch to be returned by the datastream object.\n",
    "    require_stream: str\n",
    "        One of 'train','valid' or 'test'. Specifies which part of data to query.\n",
    "    rescale_outputsize: tuple, optional\n",
    "        If this argument is not set to none, the arguments max_height_width_ratio\n",
    "        and minimum_dim_len are ignored. For details see rescale_transformer.\n",
    "        \n",
    "    \n",
    "    \"\"\" \n",
    "    if require_stream == 'train':\n",
    "        dataset = DogsVsCats(('train',),subset=slice(0,20000))\n",
    "    if require_stream == 'valid':\n",
    "        dataset = DogsVsCats(('train',),subset=slice(20000,22500))\n",
    "    if require_stream == 'test':\n",
    "        dataset = DogsVsCats(('train',),subset=slice(22500,25000))\n",
    "        \n",
    "    stream = DataStream(dataset,iteration_scheme=SequentialScheme(dataset.num_examples,batchsize))\n",
    "    \n",
    "    if rescale_outputsize == None:\n",
    "        rescale_outputsize = 'no_target'\n",
    "    \n",
    "\n",
    "    rescale_stream = rescale_transformer(stream,max_height_width_ratio,minimum_dim_len,rescale_outputsize,\n",
    "                                      which_sources=('image_features',))\n",
    "    cropped_stream = RandomFixedSizeCrop(rescale_stream,random_cropsize,which_sources=('image_features',))\n",
    "    sd_cropped_stream = ScaleAndShift(cropped_stream,1.0/255.0,0.,which_sources=('image_features',))\n",
    "    final_stream = Cast(sd_cropped_stream, dtype='float32',which_sources=('image_features',))\n",
    "\n",
    "    \n",
    "\n",
    "    return (final_stream)\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stream = get_datastream((128,128),1.5,256,128,'train',(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = next(stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.27843139,  0.28627452,  0.29411766, ...,  0.64705884,\n",
       "          0.65098041,  0.66274512],\n",
       "        [ 0.27843139,  0.29411766,  0.29411766, ...,  0.64705884,\n",
       "          0.65490198,  0.66274512],\n",
       "        [ 0.27450982,  0.28627452,  0.27843139, ...,  0.65098041,\n",
       "          0.65490198,  0.66666669],\n",
       "        ..., \n",
       "        [ 0.33725491,  0.32941177,  0.32156864, ...,  0.3764706 ,\n",
       "          0.35686275,  0.33333334],\n",
       "        [ 0.34117648,  0.33725491,  0.32941177, ...,  0.40000001,\n",
       "          0.3764706 ,  0.34117648],\n",
       "        [ 0.36078432,  0.35686275,  0.35294119, ...,  0.41176471,\n",
       "          0.3882353 ,  0.35294119]],\n",
       "\n",
       "       [[ 0.31764707,  0.32549021,  0.33333334, ...,  0.64313728,\n",
       "          0.64705884,  0.65882355],\n",
       "        [ 0.31764707,  0.33333334,  0.33333334, ...,  0.64313728,\n",
       "          0.65098041,  0.65882355],\n",
       "        [ 0.3137255 ,  0.32549021,  0.31764707, ...,  0.64705884,\n",
       "          0.65098041,  0.66274512],\n",
       "        ..., \n",
       "        [ 0.34509805,  0.33725491,  0.32941177, ...,  0.38431373,\n",
       "          0.36470589,  0.34117648],\n",
       "        [ 0.34901962,  0.34509805,  0.33725491, ...,  0.40784314,\n",
       "          0.38431373,  0.34901962],\n",
       "        [ 0.36862746,  0.36470589,  0.36078432, ...,  0.41960785,\n",
       "          0.39607844,  0.36078432]],\n",
       "\n",
       "       [[ 0.32549021,  0.33333334,  0.34117648, ...,  0.52549022,\n",
       "          0.52941179,  0.5411765 ],\n",
       "        [ 0.32549021,  0.34117648,  0.34117648, ...,  0.53333336,\n",
       "          0.5411765 ,  0.54901963],\n",
       "        [ 0.32156864,  0.33333334,  0.32549021, ...,  0.53725493,\n",
       "          0.5411765 ,  0.5529412 ],\n",
       "        ..., \n",
       "        [ 0.34117648,  0.33333334,  0.32549021, ...,  0.38039216,\n",
       "          0.36078432,  0.33725491],\n",
       "        [ 0.34509805,  0.34117648,  0.33333334, ...,  0.40392157,\n",
       "          0.38039216,  0.34509805],\n",
       "        [ 0.36470589,  0.36078432,  0.35686275, ...,  0.41568628,\n",
       "          0.39215687,  0.35686275]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff0d5caa0d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAD/CAYAAAAXKqhkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1c7ALAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0d5d85050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x[1]\n",
    "plt.imshow(image.transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 (CNMeM is enabled with initial size: 50.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.bricks import (MLP, Rectifier, Initializable, FeedforwardSequence,\n",
    "                           Softmax, Activation)\n",
    "from blocks.bricks.conv import (Convolutional, ConvolutionalSequence,\n",
    "                                Flattener, MaxPooling)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.initialization import Constant, Uniform\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from fuel.datasets import MNIST\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream\n",
    "from toolz.itertoolz import interleave\n",
    "from get_datastream import get_datastream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convolutional network example.\n",
    "\n",
    "Run the training for 50 epochs with\n",
    "```\n",
    "python convnet1.py --num-epochs 50\n",
    "```\n",
    "It is going to reach around 0.8% error rate on the test set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LeNet(FeedforwardSequence, Initializable):\n",
    "    \"\"\"LeNet-like convolutional network.\n",
    "\n",
    "    The class implements LeNet, which is a convolutional sequence with\n",
    "    an MLP on top (several fully-connected layers). For details see\n",
    "    [LeCun95]_.\n",
    "\n",
    "    .. [LeCun95] LeCun, Yann, et al.\n",
    "       *Comparison of learning algorithms for handwritten digit\n",
    "       recognition.*,\n",
    "       International conference on artificial neural networks. Vol. 60.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conv_activations : list of :class:`.Brick`\n",
    "        Activations for convolutional network.\n",
    "    num_channels : int\n",
    "        Number of channels in the input image.\n",
    "    image_shape : tuple\n",
    "        Input image shape.\n",
    "    filter_sizes : list of tuples\n",
    "        Filter sizes of :class:`.blocks.conv.ConvolutionalLayer`.\n",
    "    feature_maps : list\n",
    "        Number of filters for each of convolutions.\n",
    "    pooling_sizes : list of tuples\n",
    "        Sizes of max pooling for each convolutional layer.\n",
    "    top_mlp_activations : list of :class:`.blocks.bricks.Activation`\n",
    "        List of activations for the top MLP.\n",
    "    top_mlp_dims : list\n",
    "        Numbers of hidden units and the output dimension of the top MLP.\n",
    "    conv_step : tuples\n",
    "        Step of convolution (similar for all layers).\n",
    "    border_mode : str\n",
    "        Border mode of convolution (similar for all layers).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_activations, num_channels, image_shape,\n",
    "                 filter_sizes, feature_maps, pooling_sizes,\n",
    "                 top_mlp_activations, top_mlp_dims,\n",
    "                 conv_step=None, border_mode='valid', **kwargs):\n",
    "        if conv_step is None:\n",
    "            self.conv_step = (1, 1)\n",
    "        else:\n",
    "            self.conv_step = conv_step\n",
    "        self.num_channels = num_channels\n",
    "        self.image_shape = image_shape\n",
    "        self.top_mlp_activations = top_mlp_activations\n",
    "        self.top_mlp_dims = top_mlp_dims\n",
    "        self.border_mode = border_mode\n",
    "\n",
    "        conv_parameters = zip(filter_sizes, feature_maps)\n",
    "\n",
    "        # Construct convolutional layers with corresponding parameters\n",
    "        self.layers = list(interleave([\n",
    "            (Convolutional(filter_size=filter_size,\n",
    "                           num_filters=num_filter,\n",
    "                           step=self.conv_step,\n",
    "                           border_mode=self.border_mode,\n",
    "                           name='conv_{}'.format(i))\n",
    "             for i, (filter_size, num_filter)\n",
    "             in enumerate(conv_parameters)),\n",
    "            conv_activations,\n",
    "            (MaxPooling(size, name='pool_{}'.format(i))\n",
    "             for i, size in enumerate(pooling_sizes))]))\n",
    "\n",
    "        self.conv_sequence = ConvolutionalSequence(self.layers, num_channels,\n",
    "                                                   image_size=image_shape)\n",
    "\n",
    "        # Construct a top MLP\n",
    "        self.top_mlp = MLP(top_mlp_activations, top_mlp_dims)\n",
    "\n",
    "        # We need to flatten the output of the last convolutional layer.\n",
    "        # This brick accepts a tensor of dimension (batch_size, ...) and\n",
    "        # returns a matrix (batch_size, features)\n",
    "        self.flattener = Flattener()\n",
    "        application_methods = [self.conv_sequence.apply, self.flattener.apply,\n",
    "                               self.top_mlp.apply]\n",
    "        super(LeNet, self).__init__(application_methods, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.top_mlp_dims[-1]\n",
    "\n",
    "    @output_dim.setter\n",
    "    def output_dim(self, value):\n",
    "        self.top_mlp_dims[-1] = value\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.conv_sequence._push_allocation_config()\n",
    "        conv_out_dim = self.conv_sequence.get_dim('output')\n",
    "\n",
    "        self.top_mlp.activations = self.top_mlp_activations\n",
    "        self.top_mlp.dims = [numpy.prod(conv_out_dim)] + self.top_mlp_dims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#def main(save_to, num_epochs, feature_maps=None, \n",
    "#         mlp_hiddens=None,conv_sizes=None, pool_sizes=None, batch_size=32,\n",
    "#         from_server=False,num_batches=None,image_size=(256,256)):\n",
    "save_to = 'convnet1.tar'\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "from_server = False\n",
    "num_batches = None\n",
    "image_size = (256,256)\n",
    "feature_maps = [20, 50]\n",
    "mlp_hiddens = [500]\n",
    "conv_sizes = [5, 5]\n",
    "pool_sizes = [2, 2]\n",
    "\n",
    "output_size = 2\n",
    "\n",
    "# Use ReLUs everywhere and softmax for the final prediction\n",
    "conv_activations = [Rectifier() for _ in feature_maps]\n",
    "mlp_activations = [Rectifier() for _ in mlp_hiddens] + [Softmax()]\n",
    "convnet = LeNet(conv_activations, 3, image_size,\n",
    "                filter_sizes=zip(conv_sizes, conv_sizes),\n",
    "                feature_maps=feature_maps,\n",
    "                pooling_sizes=zip(pool_sizes, pool_sizes),\n",
    "                top_mlp_activations=mlp_activations,\n",
    "                top_mlp_dims=mlp_hiddens + [output_size],\n",
    "                border_mode='full',\n",
    "                weights_init=Uniform(width=.2),\n",
    "                biases_init=Constant(0))\n",
    "# We push initialization config to set different initialization schemes\n",
    "# for convolutional layers.\n",
    "convnet.push_initialization_config()\n",
    "convnet.layers[0].weights_init = Uniform(width=.2)\n",
    "convnet.layers[1].weights_init = Uniform(width=.09)\n",
    "convnet.top_mlp.linear_transformations[0].weights_init = Uniform(width=.08)\n",
    "convnet.top_mlp.linear_transformations[1].weights_init = Uniform(width=.11)\n",
    "convnet.initialize()\n",
    "logging.info(\"Input dim: {} {} {}\".format(\n",
    "    *convnet.children[0].get_dim('input_')))\n",
    "for i, layer in enumerate(convnet.layers):\n",
    "    if isinstance(layer, Activation):\n",
    "        logging.info(\"Layer {} ({})\".format(\n",
    "            i, layer.__class__.__name__))\n",
    "    else:\n",
    "        logging.info(\"Layer {} ({}) dim: {} {} {}\".format(\n",
    "            i, layer.__class__.__name__, *layer.get_dim('output')))\n",
    "x = tensor.tensor4('image_features')\n",
    "y = tensor.lmatrix('targets')\n",
    "\n",
    "# Normalize input and apply the convnet\n",
    "probs = convnet.apply(x)\n",
    "cost = (CategoricalCrossEntropy().apply(y.flatten(), probs)\n",
    "        .copy(name='cost'))\n",
    "error_rate = (MisclassificationRate().apply(y.flatten(), probs)\n",
    "              .copy(name='error_rate'))\n",
    "\n",
    "cg = ComputationGraph([cost, error_rate])\n",
    "if from_server:\n",
    "    train_stream = ServerDataStream(('image_features','target'),False,port=5007)\n",
    "    valid_stream = ServerDataStream(('image_features','target'),False,port=5008)\n",
    "else:\n",
    "    train_stream = get_datastream(image_size,1.5,300,batch_size,'train')\n",
    "    valid_stream = get_datastream(image_size,1.5,300,batch_size,'valid')\n",
    "\n",
    "\n",
    "# Train with simple SGD\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cg.parameters,\n",
    "    step_rule=Scale(learning_rate=0.1))\n",
    "# `Timing` extension reports time for reading data, aggregating a batch\n",
    "# and monitoring;\n",
    "# `ProgressBar` displays a nice progress bar during training.\n",
    "extensions = [Timing(),\n",
    "              FinishAfter(after_n_epochs=num_epochs,\n",
    "                          after_n_batches=num_batches),\n",
    "              DataStreamMonitoring(\n",
    "                  [cost, error_rate],\n",
    "                  valid_stream,\n",
    "                  prefix=\"valid\"),\n",
    "              TrainingDataMonitoring(\n",
    "                  [cost, error_rate,\n",
    "                   aggregation.mean(algorithm.total_gradient_norm)],\n",
    "                  prefix=\"train\",\n",
    "                  after_epoch=True),\n",
    "              Checkpoint(save_to),\n",
    "              ProgressBar(),\n",
    "              Printing()]\n",
    "\n",
    "model = Model(cost)\n",
    "\n",
    "main_loop = MainLoop(\n",
    "    algorithm,\n",
    "    train_stream,\n",
    "    model=model,\n",
    "    extensions=extensions)\n",
    "#main_loop.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = next(valid_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 256, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0, step 1 |                                        | Elapsed Time: 0:00:00ERROR:blocks.main_loop:Error occured during training.\n",
      "\n",
      "Blocks will attempt to run `on_error` extensions, potentially saving data, before exiting and reraising the error. Note that the usual `after_training` extensions will *not* be run. The original error will be re-raised and also stored in the training log. Press CTRL + C to halt Blocks immediately.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t time_initialization: 1.70327591896\n",
      "\t valid_cost: 0.813869476318\n",
      "\t valid_error_rate: 0.505537986755\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 43264000 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuDnnPool{mode='max'}(GpuContiguous.0, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 0})\nToposort index: 92\nInputs types: [CudaNdarrayType(float32, 4D), TensorType(int64, vector), TensorType(int64, vector), TensorType(int64, vector)]\nInputs shapes: [(32, 20, 260, 260), (2,), (2,), (2,)]\nInputs strides: [(1352000, 67600, 260, 1), (8,), (8,), (8,)]\nInputs values: ['not shown', array([2, 2]), array([2, 2]), array([0, 0])]\nOutputs clients: [[GpuContiguous(GpuDnnPool{mode='max'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n\nOriginal exception:\n\tMemoryError: Error allocating 43264000 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuDnnPool{mode='max'}(GpuContiguous.0, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 0})\nToposort index: 92\nInputs types: [CudaNdarrayType(float32, 4D), TensorType(int64, vector), TensorType(int64, vector), TensorType(int64, vector)]\nInputs shapes: [(32, 20, 260, 260), (2,), (2,), (2,)]\nInputs strides: [(1352000, 67600, 260, 1), (8,), (8,), (8,)]\nInputs values: ['not shown', array([2, 2]), array([2, 2]), array([0, 0])]\nOutputs clients: [[GpuContiguous(GpuDnnPool{mode='max'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-142da6a2f5b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m                     logger.error(\"Error occured when running extensions.\" +\n\u001b[0;32m    196\u001b[0m                                  error_in_error_handling_message)\n\u001b[1;32m--> 197\u001b[1;33m                 \u001b[0mreraise_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_signal_handlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/utils/__init__.pyc\u001b[0m in \u001b[0;36mreraise_as\u001b[1;34m(new_exc)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[0mnew_exc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig_exc_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0mnew_exc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_exc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_exc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_exc_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_interrupt_received'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                     \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m                         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTrainingFinish\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36m_run_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before_epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch_started'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/main_loop.pyc\u001b[0m in \u001b[0;36m_run_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before_batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'iterations_done'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/blocks/algorithms/__init__.pyc\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_source_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mordered_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mordered_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yiulau/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 43264000 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuDnnPool{mode='max'}(GpuContiguous.0, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 0})\nToposort index: 92\nInputs types: [CudaNdarrayType(float32, 4D), TensorType(int64, vector), TensorType(int64, vector), TensorType(int64, vector)]\nInputs shapes: [(32, 20, 260, 260), (2,), (2,), (2,)]\nInputs strides: [(1352000, 67600, 260, 1), (8,), (8,), (8,)]\nInputs values: ['not shown', array([2, 2]), array([2, 2]), array([0, 0])]\nOutputs clients: [[GpuContiguous(GpuDnnPool{mode='max'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n\nOriginal exception:\n\tMemoryError: Error allocating 43264000 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuDnnPool{mode='max'}(GpuContiguous.0, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 2}, TensorConstant{(2,) of 0})\nToposort index: 92\nInputs types: [CudaNdarrayType(float32, 4D), TensorType(int64, vector), TensorType(int64, vector), TensorType(int64, vector)]\nInputs shapes: [(32, 20, 260, 260), (2,), (2,), (2,)]\nInputs strides: [(1352000, 67600, 260, 1), (8,), (8,), (8,)]\nInputs values: ['not shown', array([2, 2]), array([2, 2]), array([0, 0])]\nOutputs clients: [[GpuContiguous(GpuDnnPool{mode='max'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test filter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 (CNMeM is enabled with initial size: 80.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Convolutional network example.\n",
    "\n",
    "Run the training for 50 epochs with\n",
    "```\n",
    "python convnet1.py --num-epochs 50\n",
    "```\n",
    "It is going to reach around 0.8% error rate on the test set.\n",
    "\n",
    "\"\"\"\n",
    "import logging\n",
    "import numpy\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.bricks import (MLP, Rectifier, Initializable, FeedforwardSequence,\n",
    "                           Softmax, Activation)\n",
    "from blocks.bricks.conv import (Convolutional, ConvolutionalSequence,\n",
    "                                Flattener, MaxPooling)\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.initialization import Constant, Uniform\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from fuel.datasets import MNIST\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream,ServerDataStream\n",
    "from toolz.itertoolz import interleave\n",
    "from get_datastream import get_datastream\n",
    "\n",
    "class LeNet(FeedforwardSequence, Initializable):\n",
    "    \"\"\"LeNet-like convolutional network.\n",
    "\n",
    "    The class implements LeNet, which is a convolutional sequence with\n",
    "    an MLP on top (several fully-connected layers). For details see\n",
    "    [LeCun95]_.\n",
    "\n",
    "    .. [LeCun95] LeCun, Yann, et al.\n",
    "       *Comparison of learning algorithms for handwritten digit\n",
    "       recognition.*,\n",
    "       International conference on artificial neural networks. Vol. 60.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conv_activations : list of :class:`.Brick`\n",
    "        Activations for convolutional network.\n",
    "    num_channels : int\n",
    "        Number of channels in the input image.\n",
    "    image_shape : tuple\n",
    "        Input image shape.\n",
    "    filter_sizes : list of tuples\n",
    "        Filter sizes of :class:`.blocks.conv.ConvolutionalLayer`.\n",
    "    feature_maps : list\n",
    "        Number of filters for each of convolutions.\n",
    "    pooling_sizes : list of tuples\n",
    "        Sizes of max pooling for each convolutional layer.\n",
    "    top_mlp_activations : list of :class:`.blocks.bricks.Activation`\n",
    "        List of activations for the top MLP.\n",
    "    top_mlp_dims : list\n",
    "        Numbers of hidden units and the output dimension of the top MLP.\n",
    "    conv_step : tuples\n",
    "        Step of convolution (similar for all layers).\n",
    "    border_mode : str\n",
    "        Border mode of convolution (similar for all layers).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_activations, num_channels, image_shape,\n",
    "                 filter_sizes, feature_maps, pooling_sizes,\n",
    "                 top_mlp_activations, top_mlp_dims,\n",
    "                 conv_step=None, border_mode='valid', **kwargs):\n",
    "        if conv_step is None:\n",
    "            self.conv_step = (1, 1)\n",
    "        else:\n",
    "            self.conv_step = conv_step\n",
    "        self.num_channels = num_channels\n",
    "        self.image_shape = image_shape\n",
    "        self.top_mlp_activations = top_mlp_activations\n",
    "        self.top_mlp_dims = top_mlp_dims\n",
    "        self.border_mode = border_mode\n",
    "\n",
    "        conv_parameters = zip(filter_sizes, feature_maps)\n",
    "\n",
    "        # Construct convolutional layers with corresponding parameters\n",
    "        self.layers = list(interleave([\n",
    "            (Convolutional(filter_size=filter_size,\n",
    "                           num_filters=num_filter,\n",
    "                           step=self.conv_step,\n",
    "                           border_mode=self.border_mode,\n",
    "                           name='conv_{}'.format(i))\n",
    "             for i, (filter_size, num_filter)\n",
    "             in enumerate(conv_parameters)),\n",
    "            conv_activations,\n",
    "            (MaxPooling(size, name='pool_{}'.format(i))\n",
    "             for i, size in enumerate(pooling_sizes))]))\n",
    "\n",
    "        self.conv_sequence = ConvolutionalSequence(self.layers, num_channels,\n",
    "                                                   image_size=image_shape)\n",
    "\n",
    "        # Construct a top MLP\n",
    "        self.top_mlp = MLP(top_mlp_activations, top_mlp_dims)\n",
    "\n",
    "        # We need to flatten the output of the last convolutional layer.\n",
    "        # This brick accepts a tensor of dimension (batch_size, ...) and\n",
    "        # returns a matrix (batch_size, features)\n",
    "        self.flattener = Flattener()\n",
    "        application_methods = [self.conv_sequence.apply, self.flattener.apply,\n",
    "                               self.top_mlp.apply]\n",
    "        super(LeNet, self).__init__(application_methods, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.top_mlp_dims[-1]\n",
    "\n",
    "    @output_dim.setter\n",
    "    def output_dim(self, value):\n",
    "        self.top_mlp_dims[-1] = value\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.conv_sequence._push_allocation_config()\n",
    "        conv_out_dim = self.conv_sequence.get_dim('output')\n",
    "\n",
    "        self.top_mlp.activations = self.top_mlp_activations\n",
    "        self.top_mlp.dims = [numpy.prod(conv_out_dim)] + self.top_mlp_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_maps = [20, 50]\n",
    "mlp_hiddens = [500]\n",
    "\n",
    "conv_sizes = [5, 5]\n",
    "\n",
    "pool_sizes = [2, 2]\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "output_size = 2\n",
    "image_size = (image_size,image_size) \n",
    "# Use ReLUs everywhere and softmax for the final prediction\n",
    "conv_activations = [Rectifier() for _ in feature_maps]\n",
    "mlp_activations = [Rectifier() for _ in mlp_hiddens] + [Softmax()]\n",
    "convnet = LeNet(conv_activations, 3, image_size,\n",
    "                filter_sizes=zip(conv_sizes, conv_sizes),\n",
    "                feature_maps=feature_maps,\n",
    "                pooling_sizes=zip(pool_sizes, pool_sizes),\n",
    "                top_mlp_activations=mlp_activations,\n",
    "                top_mlp_dims=mlp_hiddens + [output_size],\n",
    "                border_mode='full',\n",
    "                weights_init=Uniform(width=.2),\n",
    "                biases_init=Constant(0))\n",
    "# We push initialization config to set different initialization schemes\n",
    "# for convolutional layers.\n",
    "convnet.push_initialization_config()\n",
    "convnet.layers[0].weights_init = Uniform(width=.2)\n",
    "convnet.layers[1].weights_init = Uniform(width=.09)\n",
    "convnet.top_mlp.linear_transformations[0].weights_init = Uniform(width=.08)\n",
    "convnet.top_mlp.linear_transformations[1].weights_init = Uniform(width=.11)\n",
    "convnet.initialize()\n",
    "logging.info(\"Input dim: {} {} {}\".format(\n",
    "    *convnet.children[0].get_dim('input_')))\n",
    "for i, layer in enumerate(convnet.layers):\n",
    "    if isinstance(layer, Activation):\n",
    "        logging.info(\"Layer {} ({})\".format(\n",
    "            i, layer.__class__.__name__))\n",
    "    else:\n",
    "        logging.info(\"Layer {} ({}) dim: {} {} {}\".format(\n",
    "            i, layer.__class__.__name__, *layer.get_dim('output')))\n",
    "x = tensor.tensor4('image_features')\n",
    "y = tensor.lmatrix('targets')\n",
    "\n",
    "# Normalize input and apply the convnet\n",
    "probs = convnet.apply(x)\n",
    "cost = (CategoricalCrossEntropy().apply(y.flatten(), probs)\n",
    "        .copy(name='cost'))\n",
    "error_rate = (MisclassificationRate().apply(y.flatten(), probs)\n",
    "              .copy(name='error_rate'))\n",
    "\n",
    "cg = ComputationGraph([cost, error_rate])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import WEIGHT\n",
    "weights = VariableFilter(roles=[WEIGHT])(cg.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term1 = weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term2 = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18050,   500])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term2.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
